{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfhW3ObbMY50"
      },
      "source": [
        "### 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABESkCV3MY51",
        "outputId": "83f56d27-5d48-4671-9111-4fc8786deaae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Input, Attention, Flatten, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Bidirectional, MultiHeadAttention, LayerNormalization, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGilyFRKMY52"
      },
      "source": [
        "### 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "stj0bNBIMY53",
        "outputId": "5fc8cc81-2ed7-45cf-ebc7-5f8cdb82d40f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Churn</th>\n",
              "      <th>MonthlyRevenue</th>\n",
              "      <th>MonthlyMinutes</th>\n",
              "      <th>TotalRecurringCharge</th>\n",
              "      <th>DirectorAssistedCalls</th>\n",
              "      <th>OverageMinutes</th>\n",
              "      <th>RoamingCalls</th>\n",
              "      <th>PercChangeMinutes</th>\n",
              "      <th>PercChangeRevenues</th>\n",
              "      <th>DroppedCalls</th>\n",
              "      <th>BlockedCalls</th>\n",
              "      <th>UnansweredCalls</th>\n",
              "      <th>CustomerCareCalls</th>\n",
              "      <th>ThreewayCalls</th>\n",
              "      <th>ReceivedCalls</th>\n",
              "      <th>OutboundCalls</th>\n",
              "      <th>InboundCalls</th>\n",
              "      <th>PeakCallsInOut</th>\n",
              "      <th>OffPeakCallsInOut</th>\n",
              "      <th>DroppedBlockedCalls</th>\n",
              "      <th>CallForwardingCalls</th>\n",
              "      <th>CallWaitingCalls</th>\n",
              "      <th>MonthsInService</th>\n",
              "      <th>UniqueSubs</th>\n",
              "      <th>ActiveSubs</th>\n",
              "      <th>ServiceArea</th>\n",
              "      <th>Handsets</th>\n",
              "      <th>HandsetModels</th>\n",
              "      <th>CurrentEquipmentDays</th>\n",
              "      <th>AgeHH1</th>\n",
              "      <th>AgeHH2</th>\n",
              "      <th>ChildrenInHH</th>\n",
              "      <th>HandsetRefurbished</th>\n",
              "      <th>HandsetWebCapable</th>\n",
              "      <th>TruckOwner</th>\n",
              "      <th>RVOwner</th>\n",
              "      <th>Homeownership</th>\n",
              "      <th>BuysViaMailOrder</th>\n",
              "      <th>RespondsToMailOffers</th>\n",
              "      <th>OptOutMailings</th>\n",
              "      <th>NonUSTravel</th>\n",
              "      <th>OwnsComputer</th>\n",
              "      <th>HasCreditCard</th>\n",
              "      <th>RetentionCalls</th>\n",
              "      <th>RetentionOffersAccepted</th>\n",
              "      <th>NewCellphoneUser</th>\n",
              "      <th>NotNewCellphoneUser</th>\n",
              "      <th>ReferralsMadeBySubscriber</th>\n",
              "      <th>IncomeGroup</th>\n",
              "      <th>OwnsMotorcycle</th>\n",
              "      <th>AdjustmentsToCreditRating</th>\n",
              "      <th>HandsetPrice</th>\n",
              "      <th>MadeCallToRetentionTeam</th>\n",
              "      <th>CreditRating</th>\n",
              "      <th>PrizmCode</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>MaritalStatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3000002</td>\n",
              "      <td>Yes</td>\n",
              "      <td>24.00</td>\n",
              "      <td>219.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-157.0</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>97.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>61</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SEAPOR503</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>361.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Known</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1-Highest</td>\n",
              "      <td>Suburban</td>\n",
              "      <td>Professional</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3000010</td>\n",
              "      <td>Yes</td>\n",
              "      <td>16.99</td>\n",
              "      <td>10.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PITHOM412</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1504.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Known</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>No</td>\n",
              "      <td>4-Medium</td>\n",
              "      <td>Suburban</td>\n",
              "      <td>Professional</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3000014</td>\n",
              "      <td>No</td>\n",
              "      <td>38.00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>MILMIL414</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1812.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>No</td>\n",
              "      <td>3-Good</td>\n",
              "      <td>Town</td>\n",
              "      <td>Crafts</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3000022</td>\n",
              "      <td>No</td>\n",
              "      <td>82.28</td>\n",
              "      <td>1312.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>8.1</td>\n",
              "      <td>52.0</td>\n",
              "      <td>7.7</td>\n",
              "      <td>76.0</td>\n",
              "      <td>4.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>200.3</td>\n",
              "      <td>370.3</td>\n",
              "      <td>147.0</td>\n",
              "      <td>555.7</td>\n",
              "      <td>303.7</td>\n",
              "      <td>59.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.7</td>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>PITHOM412</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Known</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>No</td>\n",
              "      <td>4-Medium</td>\n",
              "      <td>Other</td>\n",
              "      <td>Other</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3000026</td>\n",
              "      <td>Yes</td>\n",
              "      <td>17.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>OKCTUL918</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>852.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Known</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>No</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>No</td>\n",
              "      <td>1-Highest</td>\n",
              "      <td>Other</td>\n",
              "      <td>Professional</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CustomerID Churn  MonthlyRevenue  MonthlyMinutes  TotalRecurringCharge  \\\n",
              "0     3000002   Yes           24.00           219.0                  22.0   \n",
              "1     3000010   Yes           16.99            10.0                  17.0   \n",
              "2     3000014    No           38.00             8.0                  38.0   \n",
              "3     3000022    No           82.28          1312.0                  75.0   \n",
              "4     3000026   Yes           17.14             0.0                  17.0   \n",
              "\n",
              "   DirectorAssistedCalls  OverageMinutes  RoamingCalls  PercChangeMinutes  \\\n",
              "0                   0.25             0.0           0.0             -157.0   \n",
              "1                   0.00             0.0           0.0               -4.0   \n",
              "2                   0.00             0.0           0.0               -2.0   \n",
              "3                   1.24             0.0           0.0              157.0   \n",
              "4                   0.00             0.0           0.0                0.0   \n",
              "\n",
              "   PercChangeRevenues  DroppedCalls  BlockedCalls  UnansweredCalls  \\\n",
              "0               -19.0           0.7           0.7              6.3   \n",
              "1                 0.0           0.3           0.0              2.7   \n",
              "2                 0.0           0.0           0.0              0.0   \n",
              "3                 8.1          52.0           7.7             76.0   \n",
              "4                -0.2           0.0           0.0              0.0   \n",
              "\n",
              "   CustomerCareCalls  ThreewayCalls  ReceivedCalls  OutboundCalls  \\\n",
              "0                0.0            0.0           97.2            0.0   \n",
              "1                0.0            0.0            0.0            0.0   \n",
              "2                0.0            0.0            0.4            0.3   \n",
              "3                4.3            1.3          200.3          370.3   \n",
              "4                0.0            0.0            0.0            0.0   \n",
              "\n",
              "   InboundCalls  PeakCallsInOut  OffPeakCallsInOut  DroppedBlockedCalls  \\\n",
              "0           0.0            58.0               24.0                  1.3   \n",
              "1           0.0             5.0                1.0                  0.3   \n",
              "2           0.0             1.3                3.7                  0.0   \n",
              "3         147.0           555.7              303.7                 59.7   \n",
              "4           0.0             0.0                0.0                  0.0   \n",
              "\n",
              "   CallForwardingCalls  CallWaitingCalls  MonthsInService  UniqueSubs  \\\n",
              "0                  0.0               0.3               61           2   \n",
              "1                  0.0               0.0               58           1   \n",
              "2                  0.0               0.0               60           1   \n",
              "3                  0.0              22.7               59           2   \n",
              "4                  0.0               0.0               53           2   \n",
              "\n",
              "   ActiveSubs ServiceArea  Handsets  HandsetModels  CurrentEquipmentDays  \\\n",
              "0           1   SEAPOR503       2.0            2.0                 361.0   \n",
              "1           1   PITHOM412       2.0            1.0                1504.0   \n",
              "2           1   MILMIL414       1.0            1.0                1812.0   \n",
              "3           2   PITHOM412       9.0            4.0                 458.0   \n",
              "4           2   OKCTUL918       4.0            3.0                 852.0   \n",
              "\n",
              "   AgeHH1  AgeHH2 ChildrenInHH HandsetRefurbished HandsetWebCapable  \\\n",
              "0    62.0     0.0           No                 No               Yes   \n",
              "1    40.0    42.0          Yes                 No                No   \n",
              "2    26.0    26.0          Yes                 No                No   \n",
              "3    30.0     0.0           No                 No               Yes   \n",
              "4    46.0    54.0           No                 No                No   \n",
              "\n",
              "  TruckOwner RVOwner Homeownership BuysViaMailOrder RespondsToMailOffers  \\\n",
              "0         No      No         Known              Yes                  Yes   \n",
              "1         No      No         Known              Yes                  Yes   \n",
              "2         No      No       Unknown               No                   No   \n",
              "3         No      No         Known              Yes                  Yes   \n",
              "4         No      No         Known              Yes                  Yes   \n",
              "\n",
              "  OptOutMailings NonUSTravel OwnsComputer HasCreditCard  RetentionCalls  \\\n",
              "0             No          No          Yes           Yes               1   \n",
              "1             No          No          Yes           Yes               0   \n",
              "2             No          No           No           Yes               0   \n",
              "3             No          No           No           Yes               0   \n",
              "4             No          No          Yes           Yes               0   \n",
              "\n",
              "   RetentionOffersAccepted NewCellphoneUser NotNewCellphoneUser  \\\n",
              "0                        0               No                  No   \n",
              "1                        0              Yes                  No   \n",
              "2                        0              Yes                  No   \n",
              "3                        0              Yes                  No   \n",
              "4                        0               No                 Yes   \n",
              "\n",
              "   ReferralsMadeBySubscriber  IncomeGroup OwnsMotorcycle  \\\n",
              "0                          0            4             No   \n",
              "1                          0            5             No   \n",
              "2                          0            6             No   \n",
              "3                          0            6             No   \n",
              "4                          0            9             No   \n",
              "\n",
              "   AdjustmentsToCreditRating HandsetPrice MadeCallToRetentionTeam  \\\n",
              "0                          0           30                     Yes   \n",
              "1                          0           30                      No   \n",
              "2                          0      Unknown                      No   \n",
              "3                          0           10                      No   \n",
              "4                          1           10                      No   \n",
              "\n",
              "  CreditRating PrizmCode    Occupation MaritalStatus  \n",
              "0    1-Highest  Suburban  Professional            No  \n",
              "1     4-Medium  Suburban  Professional           Yes  \n",
              "2       3-Good      Town        Crafts           Yes  \n",
              "3     4-Medium     Other         Other            No  \n",
              "4    1-Highest     Other  Professional           Yes  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 최대 열을 제한하지 않고 전부 출력\n",
        "pd.set_option('display.max_columns', None)\n",
        "df = pd.read_csv('../database/train_data/cell2celltrain.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZONUR-PMY53",
        "outputId": "301c6ff0-1831-41c7-aeab-d7c8a84d2366"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51047"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbDcReepMY53",
        "outputId": "e6acf89d-86c6-4faa-c5dd-24af5847981b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CustomerID', 'Churn', 'MonthlyRevenue', 'MonthlyMinutes',\n",
              "       'TotalRecurringCharge', 'DirectorAssistedCalls', 'OverageMinutes',\n",
              "       'RoamingCalls', 'PercChangeMinutes', 'PercChangeRevenues',\n",
              "       'DroppedCalls', 'BlockedCalls', 'UnansweredCalls', 'CustomerCareCalls',\n",
              "       'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls',\n",
              "       'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls',\n",
              "       'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService',\n",
              "       'UniqueSubs', 'ActiveSubs', 'ServiceArea', 'Handsets', 'HandsetModels',\n",
              "       'CurrentEquipmentDays', 'AgeHH1', 'AgeHH2', 'ChildrenInHH',\n",
              "       'HandsetRefurbished', 'HandsetWebCapable', 'TruckOwner', 'RVOwner',\n",
              "       'Homeownership', 'BuysViaMailOrder', 'RespondsToMailOffers',\n",
              "       'OptOutMailings', 'NonUSTravel', 'OwnsComputer', 'HasCreditCard',\n",
              "       'RetentionCalls', 'RetentionOffersAccepted', 'NewCellphoneUser',\n",
              "       'NotNewCellphoneUser', 'ReferralsMadeBySubscriber', 'IncomeGroup',\n",
              "       'OwnsMotorcycle', 'AdjustmentsToCreditRating', 'HandsetPrice',\n",
              "       'MadeCallToRetentionTeam', 'CreditRating', 'PrizmCode', 'Occupation',\n",
              "       'MaritalStatus'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 피쳐들 확인\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5cSTpKIMY54"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결측치 채우기\n",
        "\n",
        "nan_numeric_features = df.loc[:, df.isna().sum() > 0].select_dtypes(['int64', 'float64']).columns\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=10)\n",
        "df_imputed = imputer.fit_transform(df[nan_numeric_features])\n",
        "df_imputed = pd.DataFrame(df_imputed, columns=nan_numeric_features)\n",
        "df = pd.concat([df_imputed, df.drop(columns=nan_numeric_features)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3eG_o5h1MY54"
      },
      "outputs": [],
      "source": [
        "# 결측치 및 무한대 제거 (1295개)\n",
        "\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76L6skzZMY54",
        "outputId": "fd63c190-550f-4c84-b7e0-2a2c42bdecd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49752"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "u4aQNiH6MY54"
      },
      "outputs": [],
      "source": [
        "# 파생 변수 생성 - Age\n",
        "\n",
        "medianage = np.median(df['AgeHH1'])\n",
        "\n",
        "conditions = [\n",
        "    (df['AgeHH1'] == 0) & (df['AgeHH2'] == 0),\n",
        "    (df['AgeHH1'] > 0) & (df['AgeHH2'] == 0),\n",
        "    (df['AgeHH1'] > 0) & (df['AgeHH2'])]\n",
        "\n",
        "choices = [medianage, df['AgeHH1'], (df['AgeHH1'] + df['AgeHH2']) / 2]\n",
        "\n",
        "df['Age'] = np.select(conditions, choices, default= (df['AgeHH1'] + df['AgeHH2']) / 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyBxtmJtMY54",
        "outputId": "25d9fb11-7969-4797-db8e-a57724bfdaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 241 entries, 0 to 240\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   StateCode      241 non-null    object\n",
            " 1   Division       241 non-null    object\n",
            " 2   ServiceAreaNo  241 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 5.8+ KB\n"
          ]
        }
      ],
      "source": [
        "# 파생 변수 생성 - Division\n",
        "\n",
        "states = pd.read_csv('../database/train_data/state_code.csv')\n",
        "states['ServiceAreaNo'] = states['ServiceAreaNo'].astype(str)\n",
        "states.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "3DDYZcq4MY54"
      },
      "outputs": [],
      "source": [
        "states = states.drop('StateCode', axis=1)\n",
        "df['ServiceAreaNo'] = df['ServiceArea'].str.slice(start=6).astype('category')\n",
        "df = pd.merge(df, states, on = 'ServiceAreaNo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IzqoZcxPMY54"
      },
      "outputs": [],
      "source": [
        "# 파생 변수 생성 - SubsRatio\n",
        "\n",
        "df['SubsRatio'] = df['ActiveSubs'] / df['UniqueSubs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XL6r3qcMY54",
        "outputId": "efe88af2-8c41-407e-b974-a0a163ec45c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45051"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSqRz6bMY55"
      },
      "source": [
        "### 최적의 피쳐들"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "_Er1f_mGMY55"
      },
      "outputs": [],
      "source": [
        "# 중요도 높은 피쳐들\n",
        "\n",
        "important = ['MonthlyRevenue', 'TotalRecurringCharge', 'DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls', 'PercChangeRevenues', 'UnansweredCalls',\n",
        "             'CustomerCareCalls', 'ThreewayCalls', 'OutboundCalls', 'InboundCalls', 'PeakCallsInOut', 'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService',\n",
        "             'UniqueSubs', 'SubsRatio', 'Handsets', 'CurrentEquipmentDays', 'ChildrenInHH', 'HandsetRefurbished', 'HandsetWebCapable', 'BuysViaMailOrder', 'RespondsToMailOffers',\n",
        "             'HasCreditCard', 'MadeCallToRetentionTeam','CreditRating', 'PrizmCode', 'MaritalStatus', 'Division', 'Age', 'Churn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SHJj_xoqMY55"
      },
      "outputs": [],
      "source": [
        "df = df[important]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGyvcHxvMY55",
        "outputId": "4c6c2909-49e5-4ffa-b2df-5cfefca90215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "\n",
            "Index(['MonthlyRevenue', 'TotalRecurringCharge', 'DirectorAssistedCalls',\n",
            "       'OverageMinutes', 'RoamingCalls', 'PercChangeRevenues',\n",
            "       'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls',\n",
            "       'OutboundCalls', 'InboundCalls', 'PeakCallsInOut',\n",
            "       'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService',\n",
            "       'UniqueSubs', 'SubsRatio', 'Handsets', 'CurrentEquipmentDays', 'Age'],\n",
            "      dtype='object') 20\n",
            "\n",
            "Index(['ChildrenInHH', 'HandsetRefurbished', 'HandsetWebCapable',\n",
            "       'BuysViaMailOrder', 'RespondsToMailOffers', 'HasCreditCard',\n",
            "       'MadeCallToRetentionTeam', 'Churn'],\n",
            "      dtype='object') 8\n",
            "\n",
            "Index(['CreditRating', 'PrizmCode', 'MaritalStatus', 'Division'], dtype='object') 4\n"
          ]
        }
      ],
      "source": [
        "# 데이터 타입\n",
        "\n",
        "print(len(df.columns))\n",
        "print()\n",
        "numeric_cols = (df.select_dtypes(include=['number'])).columns\n",
        "print(numeric_cols, len(numeric_cols))\n",
        "print()\n",
        "binary_cols = df.columns[df.nunique() == 2]\n",
        "print(binary_cols, len(binary_cols))\n",
        "print()\n",
        "df_copy = df.copy()\n",
        "df_copy = df_copy.drop(columns=list(numeric_cols) + list(binary_cols))\n",
        "cat_cols = df_copy.columns\n",
        "print(cat_cols, len(cat_cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlPYa9flMY55",
        "outputId": "6dbb7871-2e3e-498f-e325-ab7d6809ed97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18788\\1106135952.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[feature] = df[feature].replace({'Yes' : 1, 'No' : 0})\n",
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18788\\1106135952.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['CreditRating'] = df['CreditRating'].replace(\n"
          ]
        }
      ],
      "source": [
        "# 인코딩\n",
        "\n",
        "# 바이너리\n",
        "\n",
        "for feature in binary_cols :\n",
        "    df[feature] = df[feature].replace({'Yes' : 1, 'No' : 0})\n",
        "\n",
        "\n",
        "# 범주형\n",
        "\n",
        "cat_onehot = list(cat_cols)\n",
        "cat_onehot.remove('CreditRating')\n",
        "cat_label = ['CreditRating']\n",
        "\n",
        "\n",
        "# 원-핫 인코딩\n",
        "\n",
        "df = pd.get_dummies(df, columns=cat_onehot)\n",
        "\n",
        "\n",
        "# 라벨링\n",
        "\n",
        "df['CreditRating'] = df['CreditRating'].replace(\n",
        "    {'7-Lowest' : 0,\n",
        "     '6-VeryLow' : 1,\n",
        "     '5-Low' : 2,\n",
        "     '4-Medium' : 3,\n",
        "     '3-Good' : 4,\n",
        "     '2-High' : 5,\n",
        "     '1-Highest' : 6\n",
        "     })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLg3meYrMY55",
        "outputId": "b439db7f-8ef8-41ea-a296-44b725b4d3b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['MonthlyRevenue', 'TotalRecurringCharge', 'DirectorAssistedCalls',\n",
              "       'OverageMinutes', 'RoamingCalls', 'PercChangeRevenues',\n",
              "       'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls',\n",
              "       'OutboundCalls', 'InboundCalls', 'PeakCallsInOut',\n",
              "       'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService',\n",
              "       'UniqueSubs', 'SubsRatio', 'Handsets', 'CurrentEquipmentDays',\n",
              "       'ChildrenInHH', 'HandsetRefurbished', 'HandsetWebCapable',\n",
              "       'BuysViaMailOrder', 'RespondsToMailOffers', 'HasCreditCard',\n",
              "       'MadeCallToRetentionTeam', 'CreditRating', 'Age', 'Churn',\n",
              "       'PrizmCode_Other', 'PrizmCode_Rural', 'PrizmCode_Suburban',\n",
              "       'PrizmCode_Town', 'MaritalStatus_No', 'MaritalStatus_Unknown',\n",
              "       'MaritalStatus_Yes', 'Division_midwest', 'Division_northeast',\n",
              "       'Division_south', 'Division_west'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wNKbpBtMY55",
        "outputId": "02bee2d7-973f-4dde-e94b-c88dcc7dccc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "bjMXk0lXMY55"
      },
      "outputs": [],
      "source": [
        "# 피쳐 파일 저장\n",
        "\n",
        "content = np.array(df.columns)\n",
        "\n",
        "np.savetxt('feature.txt', content, fmt='%s', delimiter=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z29uq7S1MY55"
      },
      "source": [
        "### 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "x8lKNRt-MY56"
      },
      "outputs": [],
      "source": [
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hinvzR7CMY56"
      },
      "source": [
        "### 모델 선정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFKg2031MY56",
        "outputId": "0e7980a4-d799-4ac5-c917-fcdcead9f31e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n",
            "Accuracy: 0.7169, AUC: 0.6119, F1 Score: 0.0576\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.99      0.83      6453\n",
            "           1       0.52      0.03      0.06      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.62      0.51      0.45      9011\n",
            "weighted avg       0.66      0.72      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: KNN\n",
            "Accuracy: 0.6671, AUC: 0.5540, F1 Score: 0.2643\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.85      0.78      6453\n",
            "           1       0.35      0.21      0.26      2558\n",
            "\n",
            "    accuracy                           0.67      9011\n",
            "   macro avg       0.54      0.53      0.52      9011\n",
            "weighted avg       0.62      0.67      0.64      9011\n",
            "\n",
            "============================================================\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.6107, AUC: 0.5300, F1 Score: 0.3336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.72      0.73      6453\n",
            "           1       0.32      0.34      0.33      2558\n",
            "\n",
            "    accuracy                           0.61      9011\n",
            "   macro avg       0.53      0.53      0.53      9011\n",
            "weighted avg       0.62      0.61      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: Random Forest\n",
            "Accuracy: 0.7230, AUC: 0.6482, F1 Score: 0.1423\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.98      0.83      6453\n",
            "           1       0.59      0.08      0.14      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.66      0.53      0.49      9011\n",
            "weighted avg       0.69      0.72      0.64      9011\n",
            "\n",
            "============================================================\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.7180, AUC: 0.6615, F1 Score: 0.0709\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.99      0.83      6453\n",
            "           1       0.55      0.04      0.07      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.63      0.51      0.45      9011\n",
            "weighted avg       0.67      0.72      0.62      9011\n",
            "\n",
            "============================================================\n",
            "Model: XGBoost\n",
            "Accuracy: 0.7088, AUC: 0.6460, F1 Score: 0.2429\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.92      0.82      6453\n",
            "           1       0.46      0.16      0.24      2558\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.60      0.54      0.53      9011\n",
            "weighted avg       0.66      0.71      0.66      9011\n",
            "\n",
            "============================================================\n",
            "[LightGBM] [Info] Number of positive: 10394, number of negative: 25646\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008050 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3120\n",
            "[LightGBM] [Info] Number of data points in the train set: 36040, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.288402 -> initscore=-0.903159\n",
            "[LightGBM] [Info] Start training from score -0.903159\n",
            "Model: LightGBM\n",
            "Accuracy: 0.7225, AUC: 0.6603, F1 Score: 0.1420\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.98      0.83      6453\n",
            "           1       0.58      0.08      0.14      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.65      0.53      0.49      9011\n",
            "weighted avg       0.69      0.72      0.64      9011\n",
            "\n",
            "============================================================\n",
            "Model: CatBoost\n",
            "Accuracy: 0.7254, AUC: 0.6695, F1 Score: 0.1926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.97      0.83      6453\n",
            "           1       0.58      0.12      0.19      2558\n",
            "\n",
            "    accuracy                           0.73      9011\n",
            "   macro avg       0.66      0.54      0.51      9011\n",
            "weighted avg       0.69      0.73      0.65      9011\n",
            "\n",
            "============================================================\n",
            "Model: MLP\n",
            "Accuracy: 0.7094, AUC: 0.6003, F1 Score: 0.0683\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.98      0.83      6453\n",
            "           1       0.38      0.04      0.07      2558\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.55      0.51      0.45      9011\n",
            "weighted avg       0.62      0.71      0.61      9011\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 모델 리스트\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가 결과 저장용 리스트 생성\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_proba = model.predict_proba(x_test)[:, 1] if hasattr(model, \"predict_proba\") else np.zeros(len(y_test))\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': name, 'Accuracy': accuracy, 'AUC': auc, 'F1 Score': f1})\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, F1 Score: {f1:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "TvmOnF47MY56",
        "outputId": "903a3f59-2396-40a4-95a8-c8db1c573c5a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.716902</td>\n",
              "      <td>0.611895</td>\n",
              "      <td>0.057628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.667074</td>\n",
              "      <td>0.554016</td>\n",
              "      <td>0.264345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.610698</td>\n",
              "      <td>0.529979</td>\n",
              "      <td>0.333587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.723005</td>\n",
              "      <td>0.648200</td>\n",
              "      <td>0.142268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.718011</td>\n",
              "      <td>0.661470</td>\n",
              "      <td>0.070932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.708800</td>\n",
              "      <td>0.645998</td>\n",
              "      <td>0.242931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.722450</td>\n",
              "      <td>0.660319</td>\n",
              "      <td>0.142024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.725447</td>\n",
              "      <td>0.669463</td>\n",
              "      <td>0.192559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.709355</td>\n",
              "      <td>0.600304</td>\n",
              "      <td>0.068303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy       AUC  F1 Score\n",
              "0  Logistic Regression  0.716902  0.611895  0.057628\n",
              "1                  KNN  0.667074  0.554016  0.264345\n",
              "2        Decision Tree  0.610698  0.529979  0.333587\n",
              "3        Random Forest  0.723005  0.648200  0.142268\n",
              "4    Gradient Boosting  0.718011  0.661470  0.070932\n",
              "5              XGBoost  0.708800  0.645998  0.242931\n",
              "6             LightGBM  0.722450  0.660319  0.142024\n",
              "7             CatBoost  0.725447  0.669463  0.192559\n",
              "8                  MLP  0.709355  0.600304  0.068303"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultsOG = pd.DataFrame(results)\n",
        "resultsOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 타겟 변수의 빈도 수 계산\n",
        "\n",
        "churn_counts = y_train.value_counts()\n",
        "\n",
        "\n",
        "# 막대의 위치 및 색상 설정\n",
        "\n",
        "labels = ['Not Churned (0)', 'Churned (1)']\n",
        "bars = np.arange(len(labels))\n",
        "\n",
        "\n",
        "# 히스토그램 그리기\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(bars, churn_counts, alpha=0.7, width=0.6)\n",
        "plt.xticks(bars, labels)\n",
        "\n",
        "\n",
        "# x축의 간격 조정\n",
        "\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAINCAYAAADMTOJPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6FUlEQVR4nO3de1RVdf7/8dcR9AgMnrwBUqQ0omnYZFqKVup4wUrtsmasMNQyc5aOyqhZ1rdRu2Bpkd9iUisnzGt9x2z6Ng55qbEITCWpMPI2XlJBbERQNK7794df96/zARPpeA7Q87EWa3U++733fu+zOh9f68M+G4dlWZYAAAAA2Br5ugEAAACgriEkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZ/XzfQkFRWVurIkSMKDg6Ww+HwdTsAAAAwWJalkydPKjw8XI0anX+9mJDsQUeOHFFERISv2wAAAMAFfPfdd7riiivOu52Q7EHBwcGSzr7pzZo183E3AAAAMBUVFSkiIsLObedDSPagc7dYNGvWjJAMAABQh13o1li+uAcAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAY/H3dAH6eMSlbfd0C0CAsHn2Dr1tAPcP8C3hGXZ1/WUkGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMPg3Jn3zyiYYOHarw8HA5HA699957btsty9KsWbMUHh6ugIAA9e3bVzt27HCrKSkp0cSJE9WqVSsFBQVp2LBhOnTokFtNQUGB4uPj5XK55HK5FB8frxMnTrjVHDx4UEOHDlVQUJBatWqlSZMmqbS09FJcNgAAAOo4n4bk4uJi/eY3v1FycnK12+fOnaukpCQlJydr69atCgsL08CBA3Xy5Em7JiEhQWvWrNGqVauUlpamU6dOaciQIaqoqLBr4uLilJWVpdTUVKWmpiorK0vx8fH29oqKCt1+++0qLi5WWlqaVq1apdWrV2vq1KmX7uIBAABQZ/n78uS33nqrbr311mq3WZal+fPn64knntDdd98tSVqyZIlCQ0O1YsUKjRs3ToWFhVq8eLGWLl2qAQMGSJKWLVumiIgIbdiwQbGxscrJyVFqaqo2b96sHj16SJJef/11xcTEaOfOnerYsaPWrVunb775Rt99953Cw8MlSS+++KJGjx6tZ599Vs2aNfPCuwEAAIC6wqch+afs27dPeXl5GjRokD3mdDrVp08fpaena9y4ccrMzFRZWZlbTXh4uKKjo5Wenq7Y2FhlZGTI5XLZAVmSevbsKZfLpfT0dHXs2FEZGRmKjo62A7IkxcbGqqSkRJmZmerXr1+1PZaUlKikpMR+XVRUJEkqLy9XeXm5x96Ln+KnSq+cB2jovPWZRcPB/At4hrfn35qer86G5Ly8PElSaGio23hoaKgOHDhg1zRp0kTNmzevUnNu/7y8PIWEhFQ5fkhIiFuNeZ7mzZurSZMmdk115syZo9mzZ1cZz8jIUFBQ0IUu0SN6B5/2ynmAhi4tLc3XLaCeYf4FPMPb829xcXGN6upsSD7H4XC4vbYsq8qYyayprr42NaYZM2ZoypQp9uuioiJFREQoJibGa7dorFiW6ZXzAA3diDu6+boF1DPMv4BneHv+Pfeb/wupsyE5LCxM0tlV3jZt2tjj+fn59qpvWFiYSktLVVBQ4LaanJ+fr169etk1R48erXL8Y8eOuR3n888/d9teUFCgsrKyKivMP+Z0OuV0OquM+/v7y9/fO29tBU/xAzzCW59ZNBzMv4BneHv+ren56uwnPDIyUmFhYVq/fr09Vlpaqk2bNtkBuFu3bmrcuLFbTW5urrKzs+2amJgYFRYWasuWLXbN559/rsLCQrea7Oxs5ebm2jXr1q2T0+lUt26sLgEAAPzS+HTp5NSpU9qzZ4/9et++fcrKylKLFi105ZVXKiEhQYmJiYqKilJUVJQSExMVGBiouLg4SZLL5dKYMWM0depUtWzZUi1atNC0adPUpUsX+2kXnTp10uDBgzV27FgtWrRIkvTwww9ryJAh6tixoyRp0KBB6ty5s+Lj4zVv3jwdP35c06ZN09ixY3myBQAAwC+QT0Pytm3b3J4cce7+3lGjRiklJUXTp0/XmTNnNH78eBUUFKhHjx5at26dgoOD7X1eeukl+fv7a/jw4Tpz5oz69++vlJQU+fn52TXLly/XpEmT7KdgDBs2zO3ZzH5+fvrHP/6h8ePHq3fv3goICFBcXJxeeOGFS/0WAAAAoA5yWJZl+bqJhqKoqEgul0uFhYVeW4Eek7LVK+cBGrrFo2/wdQuoZ5h/Ac/w9vxb07xWZ+9JBgAAAHyFkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAY6nRILi8v13/9138pMjJSAQEBuuqqq/TUU0+psrLSrrEsS7NmzVJ4eLgCAgLUt29f7dixw+04JSUlmjhxolq1aqWgoCANGzZMhw4dcqspKChQfHy8XC6XXC6X4uPjdeLECW9cJgAAAOqYOh2Sn3/+eS1cuFDJycnKycnR3LlzNW/ePL3yyit2zdy5c5WUlKTk5GRt3bpVYWFhGjhwoE6ePGnXJCQkaM2aNVq1apXS0tJ06tQpDRkyRBUVFXZNXFycsrKylJqaqtTUVGVlZSk+Pt6r1wsAAIC6wd/XDfyUjIwM3XHHHbr99tslSe3atdPKlSu1bds2SWdXkefPn68nnnhCd999tyRpyZIlCg0N1YoVKzRu3DgVFhZq8eLFWrp0qQYMGCBJWrZsmSIiIrRhwwbFxsYqJydHqamp2rx5s3r06CFJev311xUTE6OdO3eqY8eOPrh6AAAA+EqdDsk33XSTFi5cqF27dqlDhw768ssvlZaWpvnz50uS9u3bp7y8PA0aNMjex+l0qk+fPkpPT9e4ceOUmZmpsrIyt5rw8HBFR0crPT1dsbGxysjIkMvlsgOyJPXs2VMul0vp6ennDcklJSUqKSmxXxcVFUk6e5tIeXm5J9+K8/JT5YWLAFyQtz6zaDiYfwHP8Pb8W9Pz1emQ/Oijj6qwsFBXX321/Pz8VFFRoWeffVb33XefJCkvL0+SFBoa6rZfaGioDhw4YNc0adJEzZs3r1Jzbv+8vDyFhIRUOX9ISIhdU505c+Zo9uzZVcYzMjIUFBR0EVdae72DT3vlPEBDl5aW5usWUM8w/wKe4e35t7i4uEZ1dTokv/3221q2bJlWrFiha665RllZWUpISFB4eLhGjRpl1zkcDrf9LMuqMmYya6qrv9BxZsyYoSlTptivi4qKFBERoZiYGDVr1uyC1+cJK5ZleuU8QEM34o5uvm4B9QzzL+AZ3p5/z/3m/0LqdEh+5JFH9Nhjj+nee++VJHXp0kUHDhzQnDlzNGrUKIWFhUk6uxLcpk0be7/8/Hx7dTksLEylpaUqKChwW03Oz89Xr1697JqjR49WOf+xY8eqrFL/mNPplNPprDLu7+8vf3/vvLUVdfu7l0C94a3PLBoO5l/AM7w9/9b0fHX6E3769Gk1auTeop+fn/0IuMjISIWFhWn9+vX29tLSUm3atMkOwN26dVPjxo3danJzc5WdnW3XxMTEqLCwUFu2bLFrPv/8cxUWFto1AAAA+OWo00snQ4cO1bPPPqsrr7xS11xzjbZv366kpCQ9+OCDks7eIpGQkKDExERFRUUpKipKiYmJCgwMVFxcnCTJ5XJpzJgxmjp1qlq2bKkWLVpo2rRp6tKli/20i06dOmnw4MEaO3asFi1aJEl6+OGHNWTIEJ5sAQAA8AtUp0PyK6+8oieffFLjx49Xfn6+wsPDNW7cOP35z3+2a6ZPn64zZ85o/PjxKigoUI8ePbRu3ToFBwfbNS+99JL8/f01fPhwnTlzRv3791dKSor8/PzsmuXLl2vSpEn2UzCGDRum5ORk710sAAAA6gyHZVmWr5toKIqKiuRyuVRYWOi1L+6NSdnqlfMADd3i0Tf4ugXUM8y/gGd4e/6taV6r0/ckAwAAAL5ASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMtQrJ+/bt83QfAAAAQJ1Rq5Dcvn179evXT8uWLdMPP/zg6Z4AAAAAn6pVSP7yyy/VtWtXTZ06VWFhYRo3bpy2bNni6d4AAAAAn6hVSI6OjlZSUpIOHz6sN998U3l5ebrpppt0zTXXKCkpSceOHfN0nwAAAIDX/Kwv7vn7++uuu+7SO++8o+eff1579+7VtGnTdMUVV2jkyJHKzc31VJ8AAACA1/yskLxt2zaNHz9ebdq0UVJSkqZNm6a9e/fqo48+0uHDh3XHHXd4qk8AAADAa2oVkpOSktSlSxf16tVLR44c0VtvvaUDBw7omWeeUWRkpHr37q1Fixbpiy+++NkNHj58WPfff79atmypwMBAXXfddcrMzLS3W5alWbNmKTw8XAEBAerbt6927NjhdoySkhJNnDhRrVq1UlBQkIYNG6ZDhw651RQUFCg+Pl4ul0sul0vx8fE6ceLEz+4fAAAA9U+tQvKCBQsUFxengwcP6r333tOQIUPUqJH7oa688kotXrz4ZzVXUFCg3r17q3HjxvrnP/+pb775Ri+++KIuu+wyu2bu3LlKSkpScnKytm7dqrCwMA0cOFAnT560axISErRmzRqtWrVKaWlpOnXqlIYMGaKKigq7Ji4uTllZWUpNTVVqaqqysrIUHx//s/oHAABA/eSwLMvydRPn89hjj+mzzz7Tp59+Wu12y7IUHh6uhIQEPfroo5LOrhqHhobq+eef17hx41RYWKjWrVtr6dKluueeeyRJR44cUUREhNauXavY2Fjl5OSoc+fO2rx5s3r06CFJ2rx5s2JiYvTtt9+qY8eONeq3qKhILpdLhYWFatasmQfegQsbk7LVK+cBGrrFo2/wdQuoZ5h/Ac/w9vxb07zmX5uDv/nmm/rVr36l3//+927j//M//6PTp09r1KhRtTlsFe+//75iY2P1+9//Xps2bdLll1+u8ePHa+zYsZLO/lGTvLw8DRo0yN7H6XSqT58+Sk9P17hx45SZmamysjK3mvDwcEVHRys9PV2xsbHKyMiQy+WyA7Ik9ezZUy6XS+np6ecNySUlJSopKbFfFxUVSZLKy8tVXl7ukffgQvxU6ZXzAA2dtz6zaDiYfwHP8Pb8W9Pz1SokP/fcc1q4cGGV8ZCQED388MMeC8n//ve/tWDBAk2ZMkWPP/64tmzZokmTJsnpdGrkyJHKy8uTJIWGhrrtFxoaqgMHDkiS8vLy1KRJEzVv3rxKzbn98/LyFBISUu31nKupzpw5czR79uwq4xkZGQoKCrq4i62l3sGnvXIeoKFLS0vzdQuoZ5h/Ac/w9vxbXFxco7paheQDBw4oMjKyynjbtm118ODB2hyyWpWVlerevbsSExMlSV27dtWOHTu0YMECjRw50q5zOBxu+1mWVWXMZNZUV3+h48yYMUNTpkyxXxcVFSkiIkIxMTFeu91ixbLMCxcBuKARd3TzdQuoZ5h/Ac/w9vx77jf/F1KrkBwSEqKvvvpK7dq1cxv/8ssv1bJly9ocslpt2rRR586d3cY6deqk1atXS5LCwsIknV0JbtOmjV2Tn59vry6HhYWptLRUBQUFbqvJ+fn56tWrl11z9OjRKuc/duxYlVXqH3M6nXI6nVXG/f395e9fq7f2olX8vKf4Afg/3vrMouFg/gU8w9vzb03PV6tP+L333qtJkybp448/VkVFhSoqKvTRRx9p8uTJuvfee2tzyGr17t1bO3fudBvbtWuX2rZtK0mKjIxUWFiY1q9fb28vLS3Vpk2b7ADcrVs3NW7c2K0mNzdX2dnZdk1MTIwKCwvd/rT2559/rsLCQrsGAAAAvxy1iu7PPPOMDhw4oP79+9tpvLKyUiNHjrRvjfCEP/3pT+rVq5cSExM1fPhwbdmyRa+99ppee+01SWdvkUhISFBiYqKioqIUFRWlxMREBQYGKi4uTpLkcrk0ZswYTZ06VS1btlSLFi00bdo0denSRQMGDJB0dnV68ODBGjt2rBYtWiRJevjhhzVkyJAaP9kCAAAADUetQnKTJk309ttv6+mnn9aXX36pgIAAdenSxV7h9ZQbbrhBa9as0YwZM/TUU08pMjJS8+fP14gRI+ya6dOn68yZMxo/frwKCgrUo0cPrVu3TsHBwXbNSy+9JH9/fw0fPlxnzpxR//79lZKSIj8/P7tm+fLlmjRpkv0UjGHDhik5Odmj1wMAAID6oU4/J7m+4TnJQP3Fc5JxsZh/Ac9oUM9JrqioUEpKijZu3Kj8/HxVVro/K/Kjjz6qzWEBAACAOqFWIXny5MlKSUnR7bffrujo6As+bg0AAACoT2oVkletWqV33nlHt912m6f7AQAAAHyuVo+Aa9Kkidq3b+/pXgAAAIA6oVYheerUqfrv//5v8Z0/AAAANES1ut0iLS1NH3/8sf75z3/qmmuuUePGjd22v/vuux5pDgAAAPCFWoXkyy67THfddZenewEAAADqhFqF5DfffNPTfQAAAAB1Rq3uSZak8vJybdiwQYsWLdLJkyclSUeOHNGpU6c81hwAAADgC7VaST5w4IAGDx6sgwcPqqSkRAMHDlRwcLDmzp2rH374QQsXLvR0nwAAAIDX1GolefLkyerevbsKCgoUEBBgj991113auHGjx5oDAAAAfKHWT7f47LPP1KRJE7fxtm3b6vDhwx5pDAAAAPCVWq0kV1ZWqqKiosr4oUOHFBwc/LObAgAAAHypViF54MCBmj9/vv3a4XDo1KlTmjlzJn+qGgAAAPVerW63eOmll9SvXz917txZP/zwg+Li4rR79261atVKK1eu9HSPAAAAgFfVKiSHh4crKytLK1eu1BdffKHKykqNGTNGI0aMcPsiHwAAAFAf1SokS1JAQIAefPBBPfjgg57sBwAAAPC5WoXkt9566ye3jxw5slbNAAAAAHVBrULy5MmT3V6XlZXp9OnTatKkiQIDAwnJAAAAqNdq9XSLgoICt59Tp05p586duummm/jiHgAAAOq9WoXk6kRFRem5556rssoMAAAA1DceC8mS5OfnpyNHjnjykAAAAIDX1eqe5Pfff9/ttWVZys3NVXJysnr37u2RxgAAAABfqVVIvvPOO91eOxwOtW7dWr/97W/14osveqIvAAAAwGdqFZIrKys93QcAAABQZ3j0nmQAAACgIajVSvKUKVNqXJuUlFSbUwAAAAA+U6uQvH37dn3xxRcqLy9Xx44dJUm7du2Sn5+frr/+ervO4XB4pksAAADAi2oVkocOHarg4GAtWbJEzZs3l3T2D4w88MADuvnmmzV16lSPNgkAAAB4U63uSX7xxRc1Z84cOyBLUvPmzfXMM8/wdAsAAADUe7UKyUVFRTp69GiV8fz8fJ08efJnNwUAAAD4Uq1C8l133aUHHnhAf/vb33To0CEdOnRIf/vb3zRmzBjdfffdnu4RAAAA8Kpa3ZO8cOFCTZs2Tffff7/KysrOHsjfX2PGjNG8efM82iAAAADgbbUKyYGBgXr11Vc1b9487d27V5ZlqX379goKCvJ0fwAAAIDX/aw/JpKbm6vc3Fx16NBBQUFBsizLU30BAAAAPlOrkPyf//xH/fv3V4cOHXTbbbcpNzdXkvTQQw/x+DcAAADUe7UKyX/605/UuHFjHTx4UIGBgfb4Pffco9TUVI81BwAAAPhCre5JXrdunT788ENdccUVbuNRUVE6cOCARxoDAAAAfKVWK8nFxcVuK8jnfP/993I6nT+7KQAAAMCXahWSb7nlFr311lv2a4fDocrKSs2bN0/9+vXzWHMAAACAL9Tqdot58+apb9++2rZtm0pLSzV9+nTt2LFDx48f12effebpHgEAAACvqtVKcufOnfXVV1/pxhtv1MCBA1VcXKy7775b27dv169//WtP9wgAAAB41UWvJJeVlWnQoEFatGiRZs+efSl6AgAAAHzqoleSGzdurOzsbDkcjkvRDwAAAOBztbrdYuTIkVq8eLGnewEAAADqhFp9ca+0tFRvvPGG1q9fr+7duysoKMhte1JSkkeaAwAAAHzhokLyv//9b7Vr107Z2dm6/vrrJUm7du1yq+E2DAAAANR3FxWSo6KilJubq48//ljS2T9D/fLLLys0NPSSNAcAAAD4wkXdk2xZltvrf/7znyouLvZoQwAAAICv1eqLe+eYoRkAAABoCC4qJDscjir3HHMPMgAAABqai7on2bIsjR49Wk6nU5L0ww8/6A9/+EOVp1u8++67nusQAAAA8LKLCsmjRo1ye33//fd7tBkAAACgLriokPzmm29eqj4AAACAOuNnfXEPAAAAaIgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABjqVUieM2eOHA6HEhIS7DHLsjRr1iyFh4crICBAffv21Y4dO9z2Kykp0cSJE9WqVSsFBQVp2LBhOnTokFtNQUGB4uPj5XK55HK5FB8frxMnTnjhqgAAAFDX1JuQvHXrVr322mu69tpr3cbnzp2rpKQkJScna+vWrQoLC9PAgQN18uRJuyYhIUFr1qzRqlWrlJaWplOnTmnIkCGqqKiwa+Li4pSVlaXU1FSlpqYqKytL8fHxXrs+AAAA1B31IiSfOnVKI0aM0Ouvv67mzZvb45Zlaf78+XriiSd09913Kzo6WkuWLNHp06e1YsUKSVJhYaEWL16sF198UQMGDFDXrl21bNkyff3119qwYYMkKScnR6mpqXrjjTcUExOjmJgYvf766/rggw+0c+dOn1wzAAAAfMff1w3UxIQJE3T77bdrwIABeuaZZ+zxffv2KS8vT4MGDbLHnE6n+vTpo/T0dI0bN06ZmZkqKytzqwkPD1d0dLTS09MVGxurjIwMuVwu9ejRw67p2bOnXC6X0tPT1bFjx2r7KikpUUlJif26qKhIklReXq7y8nKPXf9P8VOlV84DNHTe+syi4WD+BTzD2/NvTc9X50PyqlWr9MUXX2jr1q1VtuXl5UmSQkND3cZDQ0N14MABu6ZJkyZuK9Dnas7tn5eXp5CQkCrHDwkJsWuqM2fOHM2ePbvKeEZGhoKCgi5wZZ7RO/i0V84DNHRpaWm+bgH1DPMv4Bnenn+Li4trVFenQ/J3332nyZMna926dWratOl56xwOh9try7KqjJnMmurqL3ScGTNmaMqUKfbroqIiRUREKCYmRs2aNfvJ83vKimWZXjkP0NCNuKObr1tAPcP8C3iGt+ffc7/5v5A6HZIzMzOVn5+vbt3+/5tXUVGhTz75RMnJyfb9wnl5eWrTpo1dk5+fb68uh4WFqbS0VAUFBW6ryfn5+erVq5ddc/To0SrnP3bsWJVV6h9zOp1yOp1Vxv39/eXv7523tqJ+3FYO1Hne+syi4WD+BTzD2/NvTc9Xpz/h/fv319dff62srCz7p3v37hoxYoSysrJ01VVXKSwsTOvXr7f3KS0t1aZNm+wA3K1bNzVu3NitJjc3V9nZ2XZNTEyMCgsLtWXLFrvm888/V2FhoV0DAACAX446vXQSHBys6Ohot7GgoCC1bNnSHk9ISFBiYqKioqIUFRWlxMREBQYGKi4uTpLkcrk0ZswYTZ06VS1btlSLFi00bdo0denSRQMGDJAkderUSYMHD9bYsWO1aNEiSdLDDz+sIUOGnPdLewAAAGi46nRIronp06frzJkzGj9+vAoKCtSjRw+tW7dOwcHBds1LL70kf39/DR8+XGfOnFH//v2VkpIiPz8/u2b58uWaNGmS/RSMYcOGKTk52evXAwAAAN9zWJZl+bqJhqKoqEgul0uFhYVe++LemJSqT/0AcPEWj77B1y2gnmH+BTzD2/NvTfNanb4nGQAAAPAFQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgqNMhec6cObrhhhsUHByskJAQ3Xnnndq5c6dbjWVZmjVrlsLDwxUQEKC+fftqx44dbjUlJSWaOHGiWrVqpaCgIA0bNkyHDh1yqykoKFB8fLxcLpdcLpfi4+N14sSJS32JAAAAqIPqdEjetGmTJkyYoM2bN2v9+vUqLy/XoEGDVFxcbNfMnTtXSUlJSk5O1tatWxUWFqaBAwfq5MmTdk1CQoLWrFmjVatWKS0tTadOndKQIUNUUVFh18TFxSkrK0upqalKTU1VVlaW4uPjvXq9AAAAqBsclmVZvm6ipo4dO6aQkBBt2rRJt9xyiyzLUnh4uBISEvToo49KOrtqHBoaqueff17jxo1TYWGhWrduraVLl+qee+6RJB05ckQRERFau3atYmNjlZOTo86dO2vz5s3q0aOHJGnz5s2KiYnRt99+q44dO9aov6KiIrlcLhUWFqpZs2aX5k0wjEnZ6pXzAA3d4tE3+LoF1DPMv4BneHv+rWle8/diTz9bYWGhJKlFixaSpH379ikvL0+DBg2ya5xOp/r06aP09HSNGzdOmZmZKisrc6sJDw9XdHS00tPTFRsbq4yMDLlcLjsgS1LPnj3lcrmUnp5+3pBcUlKikpIS+3VRUZEkqby8XOXl5Z678J/gp0qvnAdo6Lz1mUXDwfwLeIa359+anq/ehGTLsjRlyhTddNNNio6OliTl5eVJkkJDQ91qQ0NDdeDAAbumSZMmat68eZWac/vn5eUpJCSkyjlDQkLsmurMmTNHs2fPrjKekZGhoKCgi7i62usdfNor5wEaurS0NF+3gHqG+RfwDG/Pvz++bfen1JuQ/Mc//lFfffVVtW+kw+Fwe21ZVpUxk1lTXf2FjjNjxgxNmTLFfl1UVKSIiAjFxMR47XaLFcsyvXIeoKEbcUc3X7eAeob5F/AMb8+/537zfyH1IiRPnDhR77//vj755BNdccUV9nhYWJiksyvBbdq0scfz8/Pt1eWwsDCVlpaqoKDAbTU5Pz9fvXr1smuOHj1a5bzHjh2rskr9Y06nU06ns8q4v7+//P2989ZW1O3vXgL1hrc+s2g4mH8Bz/D2/FvT89XpT7hlWfrjH/+od999Vx999JEiIyPdtkdGRiosLEzr16+3x0pLS7Vp0yY7AHfr1k2NGzd2q8nNzVV2drZdExMTo8LCQm3ZssWu+fzzz1VYWGjXAAAA4JejTi+dTJgwQStWrNDf//53BQcH2/cHu1wuBQQEyOFwKCEhQYmJiYqKilJUVJQSExMVGBiouLg4u3bMmDGaOnWqWrZsqRYtWmjatGnq0qWLBgwYIEnq1KmTBg8erLFjx2rRokWSpIcfflhDhgyp8ZMtAAAA0HDU6ZC8YMECSVLfvn3dxt98802NHj1akjR9+nSdOXNG48ePV0FBgXr06KF169YpODjYrn/ppZfk7++v4cOH68yZM+rfv79SUlLk5+dn1yxfvlyTJk2yn4IxbNgwJScnX9oLBAAAQJ1Ur56TXNfxnGSg/uI5ybhYzL+AZ9TV5yTX6XuSAQAAAF8gJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQrLh1VdfVWRkpJo2bapu3brp008/9XVLAAAA8DJC8o+8/fbbSkhI0BNPPKHt27fr5ptv1q233qqDBw/6ujUAAAB4ESH5R5KSkjRmzBg99NBD6tSpk+bPn6+IiAgtWLDA160BAADAi/x93UBdUVpaqszMTD322GNu44MGDVJ6enq1+5SUlKikpMR+XVhYKEk6fvy4ysvLL12zP1Jx5qRXzgM0dMePH/d1C6hnmH8Bz/D2/FtUVCRJsizrJ+sIyf/n+++/V0VFhUJDQ93GQ0NDlZeXV+0+c+bM0ezZs6uMR0ZGXpIeAVw6S8b7ugMA+GXy1fx78uRJuVyu824nJBscDofba8uyqoydM2PGDE2ZMsV+XVlZqePHj6tly5bn3Qe/PEVFRYqIiNB3332nZs2a+bodAPjFYP5FdSzL0smTJxUeHv6TdYTk/9OqVSv5+flVWTXOz8+vsrp8jtPplNPpdBu77LLLLlWLqOeaNWvGJA0APsD8C9NPrSCfwxf3/k+TJk3UrVs3rV+/3m18/fr16tWrl4+6AgAAgC+wkvwjU6ZMUXx8vLp3766YmBi99tprOnjwoP7whz/4ujUAAAB4ESH5R+655x795z//0VNPPaXc3FxFR0dr7dq1atu2ra9bQz3mdDo1c+bMKrfmAAAuLeZf/BwO60LPvwAAAAB+YbgnGQAAADAQkgEAAAADIRkAAAAwEJKBC/jXv/4lh8OhEydO+LqVi+JwOPTee+/9ZM1//vMfhYSEaP/+/TU+7gcffKCuXbuqsrLy5zUI4BejJvNRXdO3b18lJCRcsO6WW27RihUranzc/Px8tW7dWocPH/4Z3cEbCMmoM0aPHi2Hw6HnnnvObfy999676L9g2K5dO82fP79Gtdu3b9fvf/97hYaGqmnTpurQoYPGjh2rXbt2XdQ566M5c+Zo6NChateunT128OBBDR06VEFBQWrVqpUmTZqk0tJSe/uQIUPkcDgu6h8FAA1XXl6eJk6cqKuuukpOp1MREREaOnSoNm7c6OvWLrkPPvhAeXl5uvfee+2x1157TX379lWzZs2qXWAJCQlRfHy8Zs6c6eVucbEIyahTmjZtqueff14FBQVeOd8HH3ygnj17qqSkRMuXL1dOTo6WLl0ql8ulJ5988pKe+8fB0xfOnDmjxYsX66GHHrLHKioqdPvtt6u4uFhpaWlatWqVVq9eralTp7rt+8ADD+iVV17xdssA6pj9+/erW7du+uijjzR37lx9/fXXSk1NVb9+/TRhwoRLeu6ysrJLevyaePnll/XAAw+oUaP/H6dOnz6twYMH6/HHHz/vfg888ICWL1/utX/rUEsWUEeMGjXKGjJkiHX11VdbjzzyiD2+Zs0ay/xf9W9/+5vVuXNnq0mTJlbbtm2tF154wd7Wp08fS5LbT3WKi4utVq1aWXfeeWe12wsKCizLsqyPP/7YkmRt2LDB6tatmxUQEGDFxMRY3377rVvvd9xxh9v+kydPtvr06ePW14QJE6w//elPVsuWLa1bbrmlRse2LMt6//33reuvv95yOp1WZGSkNWvWLKusrMzevmvXLuvmm2+2nE6n1alTJ2vdunWWJGvNmjXVXptlWdbq1autVq1auY2tXbvWatSokXX48GF7bOXKlZbT6bQKCwvtsf3791uSrL179573+AAavltvvdW6/PLLrVOnTlXZdm4OtSzLkmS9/vrr1p133mkFBARY7du3t/7+97/b2998803L5XK57W/O/TNnzrR+85vfWIsXL7YiIyMth8NhVVZWXvDYlmVZO3bssG699VYrKCjICgkJse6//37r2LFj9vZTp05Z8fHxVlBQkBUWFma98MILVp8+fazJkyef99qPHTtmORwOKzs7u9rt5+b3H78PP9auXTtr8eLF5z0+fI+VZNQpfn5+SkxM1CuvvKJDhw5VW5OZmanhw4fr3nvv1ddff61Zs2bpySefVEpKiiTp3Xff1RVXXGH/UZjc3Nxqj/Phhx/q+++/1/Tp06vdftlll7m9fuKJJ/Tiiy9q27Zt8vf314MPPnjR17dkyRL5+/vrs88+06JFi2p07A8//FD333+/Jk2apG+++UaLFi1SSkqKnn32WUlSZWWl7r77bvn5+Wnz5s1auHChHn300Qv28sknn6h79+5uYxkZGYqOjlZ4eLg9Fhsbq5KSEmVmZtpjbdu2VUhIiD799NOLfg8ANAzHjx9XamqqJkyYoKCgoCrbzTl09uzZGj58uL766ivddtttGjFihI4fP35R59yzZ4/eeecdrV69WllZWTU6dm5urvr06aPrrrtO27ZtU2pqqo4eParhw4fb+z/yyCP6+OOPtWbNGq1bt07/+te/3Oa86qSlpSkwMFCdOnW6qGs458Ybb2QOreP4i3uoc+666y5dd911mjlzphYvXlxle1JSkvr372/fDtGhQwd98803mjdvnkaPHq0WLVrIz89PwcHBCgsLO+95du/eLUm6+uqra9TXs88+qz59+kiSHnvsMd1+++364Ycf1LRp0xpfW/v27TV37lz7dV5e3gWP/eyzz+qxxx7TqFGjJElXXXWVnn76aU2fPl0zZ87Uhg0blJOTo/379+uKK66QJCUmJurWW2/9yV7279/vFobP9RMaGuo21rx5czVp0sTu9ZzLL7/8or7wB6Bh2bNnjyzLqvEcOnr0aN13332SZC+GbNmyRYMHD67xOUtLS7V06VK1bt26xsdesGCBrr/+eiUmJtr1f/3rXxUREaFdu3YpPDxcixcv1ltvvaWBAwdKOrugcW4+PZ/9+/crNDTU7VaLi3H55Zdr+/bttdoX3sFKMuqk559/XkuWLNE333xTZVtOTo569+7tNta7d2/t3r1bFRUVNT6HdZF/bPLaa6+1/7tNmzaSzn5L+WKYK7c1OXZmZqaeeuop/epXv7J/xo4dq9zcXJ0+fVo5OTm68sor3Sb0mJiYC/Zy5syZagN+dV+StCyrynhAQIBOnz59wfMAaJjOzaE1/WL1j+e5oKAgBQcHX/Qc2rZt2yoB+ULHzszM1Mcff+w2h54L9nv37tXevXtVWlrqNm+2aNFCHTt2/MlezjeH1hRzaN1HSEaddMsttyg2NrbaLz5UF9guNvBKZ1egJenbb7+tUX3jxo3t/z53/nOPQWvUqFGVHqr7Ukl1v5K80LErKys1e/ZsZWVl2T9ff/21du/eraZNm1Z77TX5R6tVq1ZVvjQSFhZWZcW4oKBAZWVlVVaYjx8/Xu0/VgB+GaKiouRwOJSTk1Oj+h/Pc9LZeepSzKHmsSsrKzV06FC3OTQrK0u7d+/WLbfcUqt/P6Tq59CLwRxa9xGSUWc999xz+t///V+lp6e7jXfu3FlpaWluY+np6erQoYP8/PwkSU2aNLngqvKgQYPUqlUrt9sffuxinovcunXrKvc+//h+uZ/j+uuv186dO9W+ffsqP40aNVLnzp118OBBHTlyxN4nIyPjgsft2rVrlZX6mJgYZWdnu13LunXr5HQ61a1bN3vshx9+0N69e9W1a1cPXCGA+qhFixaKjY3VX/7yFxUXF1fZfrFz6MmTJ92O48k5dMeOHWrXrl2VOTQoKEjt27dX48aNtXnzZnufgoKCCz4GtGvXrsrLy6t1UM7OzmYOreMIyaizunTpohEjRlR51NjUqVO1ceNGPf3009q1a5eWLFmi5ORkTZs2za5p166dPvnkEx0+fFjff/99tccPCgrSG2+8oX/84x8aNmyYNmzYoP3792vbtm2aPn26/vCHP9S419/+9rfatm2b3nrrLe3evVszZ85UdnZ27S7c8Oc//1lvvfWWZs2apR07dignJ0dvv/22/uu//kuSNGDAAHXs2FEjR47Ul19+qU8//VRPPPHEBY8bGxurHTt2uE3wgwYNUufOnRUfH6/t27dr48aNmjZtmsaOHatmzZrZdZs3b5bT6azRbR0AGq5XX31VFRUVuvHGG7V69Wrt3r1bOTk5evnlly9qfujRo4cCAwP1+OOPa8+ePVqxYoX9Zeyfa8KECTp+/Ljuu+8+bdmyRf/+97+1bt06Pfjgg6qoqNCvfvUrjRkzRo888og2btyo7OxsjR49+oL3Gnft2lWtW7fWZ5995jael5enrKws7dmzR5L09ddfKysry+1LiqdPn1ZmZqYGDRrkkWvEpUFIRp329NNPV/lV2PXXX6933nlHq1atUnR0tP785z/rqaee0ujRo+2ap556Svv379evf/3rn/x11h133KH09HQ1btxYcXFxuvrqq3XfffepsLBQzzzzTI37jI2N1ZNPPqnp06frhhtu0MmTJzVy5MiLvt7zHfuDDz7Q+vXrdcMNN6hnz55KSkpS27ZtJZ39NeWaNWtUUlKiG2+8UQ899JD95Iuf0qVLF3Xv3l3vvPOOPebn56d//OMfatq0qXr37q3hw4frzjvv1AsvvOC278qVKzVixAgFBgZ65BoB1E+RkZH64osv1K9fP02dOlXR0dEaOHCgNm7cqAULFtT4OC1atNCyZcu0du1adenSRStXrtSsWbM80mN4eLg+++wzVVRUKDY2VtHR0Zo8ebJcLpcdhOfNm6dbbrlFw4YN04ABA3TTTTe5/fasOn5+fnrwwQe1fPlyt/GFCxeqa9euGjt2rKSztw927dpV77//vl3z97//XVdeeaVuvvlmj1wjLg2HVdubcQDUe2vXrtW0adOUnZ1d429oHzt2TFdffbW2bdumyMjIS9whANRdR48e1TXXXKPMzEx74aImbrzxRiUkJCguLu4Sdoefi0fAAb9gt912m3bv3q3Dhw8rIiKiRvvs27dPr776KgEZwC9eaGioFi9erIMHD9Y4JOfn5+t3v/ud/cg61F2sJAMAAAAG7kkGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQA+IVyOBx67733fN0GANRJhGQAaKDy8vI0ceJEXXXVVXI6nYqIiNDQoUO1ceNGX7cGAHUef0wEABqg/fv3q3fv3rrssss0d+5cXXvttSorK9OHH36oCRMm6Ntvv70k5y0rK1Pjxo0vybEBwJtYSQaABmj8+PFyOBzasmWLfve736lDhw665pprNGXKFG3evNmu+/7773XXXXcpMDBQUVFRev/99+1tKSkpuuyyy9yO+95778nhcNivZ82apeuuu05//etf7RVry7LkcDj0xhtvnPfYAFDXEZIBoIE5fvy4UlNTNWHCBAUFBVXZ/uPgO3v2bA0fPlxfffWVbrvtNo0YMULHjx+/qPPt2bNH77zzjlavXq2srCyPHhsAfIWQDAANzJ49e2RZlq6++uoL1o4ePVr33Xef2rdvr8TERBUXF2vLli0Xdb7S0lItXbpUXbt21bXXXmuvNHvi2ADgK4RkAGhgLMuSJLfbIs7n2muvtf87KChIwcHBys/Pv6jztW3bVq1bt74kxwYAXyEkA0ADExUVJYfDoZycnAvWml+yczgcqqyslCQ1atTIDtznlJWVVTlGdbd0XOjYAFDXEZIBoIFp0aKFYmNj9Ze//EXFxcVVtp84caJGx2ndurVOnjzpdowf33MMAA0ZIRkAGqBXX31VFRUVuvHGG7V69Wrt3r1bOTk5evnllxUTE1OjY/To0UOBgYF6/PHHtWfPHq1YsUIpKSmXtnEAqCMIyQDQAEVGRuqLL75Qv379NHXqVEVHR2vgwIHauHGjFixYUKNjtGjRQsuWLdPatWvVpUsXrVy5UrNmzbq0jQNAHeGwzBvOAAAAgF84VpIBAAAAAyEZAAAAMBCSAQAAAAMhGQAAADAQkgEAAAADIRkAAAAwEJIBAAAAAyEZAAAAMBCSAQAAAAMhGQAAADAQkgEAAAADIRkAAAAw/D8+KGpG3/+vegAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 언더 샘플링\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "x_train_resampled, y_train_resampled = undersampler.fit_resample(x_train, y_train)\n",
        "\n",
        "churn_counts_resampled = y_train_resampled.value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(bars, churn_counts_resampled, alpha=0.7, width=0.6)\n",
        "plt.xticks(bars, labels)\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAINCAYAAADMTOJPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA74UlEQVR4nO3de1yUdf7//+cEMiqLo4SAbISWShq2a1iKlodUUBNTP7tWFkmZtTddDyusu9W3TSu11KxPuZnbup6N2jWrTSPNTkuAJkmFkWmrqQliiaBkHK/fH328fs4bD4jjDNLjfrtxuznv6zXv6zWzy7vn7c011zgsy7IEAAAAwHaJrxsAAAAAGhpCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGDw93UDjUlNTY0OHDigoKAgORwOX7cDAAAAg2VZOnr0qCIiInTJJaffLyYke9CBAwcUGRnp6zYAAABwFvv27dNll1122uOEZA8KCgqS9NOb3qJFCx93AwAAAFNpaakiIyPt3HY6hGQPOnGJRYsWLQjJAAAADdjZLo3lg3sAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAwd/XDeD8jF36sa9bABqFxcnX+boFXGRYfwHPaKjrLzvJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGHwakmfPnq3rrrtOQUFBCg0N1fDhw7Vjxw63muTkZDkcDrefHj16uNWUl5dr4sSJCgkJUWBgoIYNG6b9+/e71RQXFyspKUkul0sul0tJSUk6cuSIW83evXuVmJiowMBAhYSEaNKkSaqoqLggrx0AAAANl09D8gcffKAJEyYoOztbGzduVFVVleLj41VWVuZWN2jQIBUUFNg/69evdzs+ZcoUrV27VmlpacrIyNCxY8c0dOhQVVdX2zWjR49Wbm6u0tPTlZ6ertzcXCUlJdnHq6urdfPNN6usrEwZGRlKS0vTmjVrlJKScmHfBAAAADQ4Pv0ykfT0dLfHS5YsUWhoqHJyctS7d2973Ol0Kjw8/JRzlJSUaPHixVqxYoUGDBggSVq5cqUiIyP1zjvvKCEhQfn5+UpPT1d2dra6d+8uSXrxxRcVFxenHTt2KDo6Whs2bNAXX3yhffv2KSIiQpL01FNPKTk5WTNnzlSLFi0uxFsAAACABqhBfeNeSUmJJCk4ONht/P3331doaKhatmypPn36aObMmQoNDZUk5eTkqLKyUvHx8XZ9RESEYmJilJmZqYSEBGVlZcnlctkBWZJ69Oghl8ulzMxMRUdHKysrSzExMXZAlqSEhASVl5crJydH/fr1q9VveXm5ysvL7celpaWSpKqqKlVVVXngHTk7P9V45TxAY+et31k0Hqy/gGd4e/2t6/kaTEi2LEtTp07VDTfcoJiYGHt88ODB+u1vf6uoqCjt3r1bDz/8sG666Sbl5OTI6XSqsLBQAQEBatWqldt8YWFhKiwslCQVFhbaofpkoaGhbjVhYWFux1u1aqWAgAC7xjR79mzNmDGj1nhWVpYCAwPP7Q2op15BP3jlPEBjl5GR4esWcJFh/QU8w9vrr3lZ7+k0mJD8+9//Xp999lmtN+rWW2+1/x0TE6Nu3bopKipK69at08iRI087n2VZcjgc9uOT/30+NSd74IEHNHXqVPtxaWmpIiMjFRcX57XLM1avzPHKeYDG7o5bYn3dAi4yrL+AZ3h7/T3xl/+zaRAheeLEiXrjjTf04Ycf6rLLLjtjbZs2bRQVFaWdO3dKksLDw1VRUaHi4mK33eSioiL17NnTrjl48GCtuQ4dOmTvHoeHh2vz5s1ux4uLi1VZWVlrh/kEp9Mpp9NZa9zf31/+/t55a6u5ix/gEd76nUXjwfoLeIa319+6ns+nv+GWZen3v/+9Xn31Vb377rtq167dWZ/z/fffa9++fWrTpo0kKTY2Vk2aNNHGjRvtmoKCAuXl5dkhOS4uTiUlJdqyZYtds3nzZpWUlLjV5OXlqaCgwK7ZsGGDnE6nYmPZYQIAAPg58enWyYQJE7R69Wq9/vrrCgoKsq/9dblcatasmY4dO6bp06frf/7nf9SmTRvt2bNHDz74oEJCQjRixAi7duzYsUpJSdGll16q4OBgpaamqkuXLvbdLjp16qRBgwZp3LhxWrRokSTpvvvu09ChQxUdHS1Jio+PV+fOnZWUlKS5c+fq8OHDSk1N1bhx47izBQAAwM+MT3eSFy5cqJKSEvXt21dt2rSxf15++WVJkp+fnz7//HPdcsst6tixo8aMGaOOHTsqKytLQUFB9jxPP/20hg8frlGjRqlXr15q3ry5/v3vf8vPz8+uWbVqlbp06aL4+HjFx8frmmuu0YoVK+zjfn5+WrdunZo2bapevXpp1KhRGj58uObNm+e9NwQAAAANgsOyLMvXTTQWpaWlcrlcKikp8dru89ilH3vlPEBjtzj5Ol+3gIsM6y/gGd5ef+ua1/jUAQAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGn4bk2bNn67rrrlNQUJBCQ0M1fPhw7dixw63GsixNnz5dERERatasmfr27avt27e71ZSXl2vixIkKCQlRYGCghg0bpv3797vVFBcXKykpSS6XSy6XS0lJSTpy5Ihbzd69e5WYmKjAwECFhIRo0qRJqqiouCCvHQAAAA2XT0PyBx98oAkTJig7O1sbN25UVVWV4uPjVVZWZtfMmTNH8+fP14IFC/Txxx8rPDxcAwcO1NGjR+2aKVOmaO3atUpLS1NGRoaOHTumoUOHqrq62q4ZPXq0cnNzlZ6ervT0dOXm5iopKck+Xl1drZtvvlllZWXKyMhQWlqa1qxZo5SUFO+8GQAAAGgwHJZlWb5u4oRDhw4pNDRUH3zwgXr37i3LshQREaEpU6boT3/6k6Sfdo3DwsL05JNP6v7771dJSYlat26tFStW6NZbb5UkHThwQJGRkVq/fr0SEhKUn5+vzp07Kzs7W927d5ckZWdnKy4uTl9++aWio6P11ltvaejQodq3b58iIiIkSWlpaUpOTlZRUZFatGhx1v5LS0vlcrlUUlJSp3pPGLv0Y6+cB2jsFidf5+sWcJFh/QU8w9vrb13zWoO6JrmkpESSFBwcLEnavXu3CgsLFR8fb9c4nU716dNHmZmZkqScnBxVVla61URERCgmJsauycrKksvlsgOyJPXo0UMul8utJiYmxg7IkpSQkKDy8nLl5ORcoFcMAACAhsjf1w2cYFmWpk6dqhtuuEExMTGSpMLCQklSWFiYW21YWJi++eYbuyYgIECtWrWqVXPi+YWFhQoNDa11ztDQULca8zytWrVSQECAXWMqLy9XeXm5/bi0tFSSVFVVpaqqqrq98PPkpxqvnAdo7Lz1O4vGg/UX8Axvr791PV+DCcm///3v9dlnnykjI6PWMYfD4fbYsqxaYyaz5lT19ak52ezZszVjxoxa41lZWQoMDDxjf57SK+gHr5wHaOxOtfYAZ8L6C3iGt9ffkz/7diYNIiRPnDhRb7zxhj788ENddtll9nh4eLikn3Z527RpY48XFRXZu77h4eGqqKhQcXGx225yUVGRevbsadccPHiw1nkPHTrkNs/mzZvdjhcXF6uysrLWDvMJDzzwgKZOnWo/Li0tVWRkpOLi4rx2TfLqlVwKAnjCHbfE+roFXGRYfwHP8Pb6e+Iv/2fj05BsWZYmTpyotWvX6v3331e7du3cjrdr107h4eHauHGjunbtKkmqqKjQBx98oCeffFKSFBsbqyZNmmjjxo0aNWqUJKmgoEB5eXmaM2eOJCkuLk4lJSXasmWLrr/+eknS5s2bVVJSYgfpuLg4zZw5UwUFBXYg37Bhg5xOp2JjT/0/ntPplNPprDXu7+8vf3/vvLXVDeuycuCi5a3fWTQerL+AZ3h7/a3r+Xz6X4UJEyZo9erVev311xUUFGRf++tyudSsWTM5HA5NmTJFs2bNUocOHdShQwfNmjVLzZs31+jRo+3asWPHKiUlRZdeeqmCg4OVmpqqLl26aMCAAZKkTp06adCgQRo3bpwWLVokSbrvvvs0dOhQRUdHS5Li4+PVuXNnJSUlae7cuTp8+LBSU1M1btw4r+0KAwAAoGHwaUheuHChJKlv375u40uWLFFycrIkadq0aTp+/LjGjx+v4uJide/eXRs2bFBQUJBd//TTT8vf31+jRo3S8ePH1b9/fy1dulR+fn52zapVqzRp0iT7LhjDhg3TggUL7ON+fn5at26dxo8fr169eqlZs2YaPXq05s2bd4FePQAAABqqBnWf5Isd90kGLl7cJxnnivUX8AzukwwAAABcJAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGOoVknfv3u3pPgAAAIAGo14huX379urXr59WrlypH3/80dM9AQAAAD5Vr5D86aefqmvXrkpJSVF4eLjuv/9+bdmy5Zzn+fDDD5WYmKiIiAg5HA699tprbseTk5PlcDjcfnr06OFWU15erokTJyokJESBgYEaNmyY9u/f71ZTXFyspKQkuVwuuVwuJSUl6ciRI241e/fuVWJiogIDAxUSEqJJkyapoqLinF8TAAAALn71CskxMTGaP3++vv32Wy1ZskSFhYW64YYbdPXVV2v+/Pk6dOhQneYpKyvTr371Ky1YsOC0NYMGDVJBQYH9s379erfjU6ZM0dq1a5WWlqaMjAwdO3ZMQ4cOVXV1tV0zevRo5ebmKj09Xenp6crNzVVSUpJ9vLq6WjfffLPKysqUkZGhtLQ0rVmzRikpKef4zgAAAKAxcFiWZZ3vJOXl5Xr++ef1wAMPqKKiQk2aNNGtt96qJ598Um3atKlbIw6H1q5dq+HDh9tjycnJOnLkSK0d5hNKSkrUunVrrVixQrfeeqsk6cCBA4qMjNT69euVkJCg/Px8de7cWdnZ2erevbskKTs7W3Fxcfryyy8VHR2tt956S0OHDtW+ffsUEREhSUpLS1NycrKKiorUokWLOr2G0tJSuVwulZSU1Pk552vs0o+9ch6gsVucfJ2vW8BFhvUX8Axvr791zWv+53OSrVu36h//+IfS0tIUGBio1NRUjR07VgcOHNBf/vIX3XLLLfW6DONk77//vkJDQ9WyZUv16dNHM2fOVGhoqCQpJydHlZWVio+Pt+sjIiIUExOjzMxMJSQkKCsrSy6Xyw7IktSjRw+5XC5lZmYqOjpaWVlZiomJsQOyJCUkJKi8vFw5OTnq16/fKXsrLy9XeXm5/bi0tFSSVFVVpaqqqvN63XXlpxqvnAdo7Lz1O4vGg/UX8Axvr791PV+9QvL8+fO1ZMkS7dixQ0OGDNHy5cs1ZMgQXXLJT1dvtGvXTosWLdJVV11Vn+ltgwcP1m9/+1tFRUVp9+7devjhh3XTTTcpJydHTqdThYWFCggIUKtWrdyeFxYWpsLCQklSYWGhHapPFhoa6lYTFhbmdrxVq1YKCAiwa05l9uzZmjFjRq3xrKwsBQYGnvPrrY9eQT945TxAY5eRkeHrFnCRYf0FPMPb629ZWVmd6uoVkhcuXKh77rlHd999t8LDw09Zc/nll2vx4sX1md524hIK6afroLt166aoqCitW7dOI0eOPO3zLMuSw+GwH5/87/OpMT3wwAOaOnWq/bi0tFSRkZGKi4vz2uUWq1fmeOU8QGN3xy2xvm4BFxnWX8AzvL3+nvjL/9nUKyTv3LnzrDUBAQEaM2ZMfaY/rTZt2igqKso+f3h4uCoqKlRcXOy2m1xUVKSePXvaNQcPHqw116FDh+zd4/DwcG3evNnteHFxsSorK2vtMJ/M6XTK6XTWGvf395e//3ldyVJn1XwfDOAR3vqdRePB+gt4hrfX37qer16/4UuWLNE///nPWuP//Oc/tWzZsvpMWSfff/+99u3bZ38YMDY2Vk2aNNHGjRvtmoKCAuXl5dkhOS4uTiUlJW7XRm/evFklJSVuNXl5eSooKLBrNmzYIKfTqdhYdpcAAAB+buoVkp944gmFhITUGg8NDdWsWbPqPM+xY8eUm5ur3NxcST99k19ubq727t2rY8eOKTU1VVlZWdqzZ4/ef/99JSYmKiQkRCNGjJAkuVwujR07VikpKdq0aZO2bdumO++8U126dNGAAQMkSZ06ddKgQYM0btw4ZWdnKzs7W+PGjdPQoUMVHR0tSYqPj1fnzp2VlJSkbdu2adOmTUpNTdW4ceO8dtkEAAAAGo567W9/8803ateuXa3xqKgo7d27t87zbN261e3OESeu7x0zZowWLlyozz//XMuXL9eRI0fUpk0b9evXTy+//LKCgoLs5zz99NPy9/fXqFGjdPz4cfXv319Lly6Vn5+fXbNq1SpNmjTJvgvGsGHD3O7N7Ofnp3Xr1mn8+PHq1auXmjVrptGjR2vevHl1f1MAAADQaNQrJIeGhuqzzz5T27Zt3cY//fRTXXrppXWep2/fvjrTbZrffvvts87RtGlTPffcc3ruuedOWxMcHKyVK1eecZ7LL79cb7755lnPBwAAgMavXpdb3HbbbZo0aZLee+89VVdXq7q6Wu+++64mT56s2267zdM9AgAAAF5Vr53kxx9/XN9884369+9vf0KwpqZGd9111zldkwwAAAA0RPUKyQEBAXr55Zf12GOP6dNPP1WzZs3UpUsXRUVFebo/AAAAwOvO68Z0HTt2VMeOHT3VCwAAANAg1CskV1dXa+nSpdq0aZOKiopUU+P+/fXvvvuuR5oDAAAAfKFeIXny5MlaunSpbr75ZsXExJzxq5sBAACAi029QnJaWppeeeUVDRkyxNP9AAAAAD5Xr1vABQQEqH379p7uBQAAAGgQ6hWSU1JS9L//+79n/CIQAAAA4GJVr8stMjIy9N577+mtt97S1VdfrSZNmrgdf/XVVz3SHAAAAOAL9QrJLVu21IgRIzzdCwAAANAg1CskL1myxNN9AAAAAA1Gva5JlqSqqiq98847WrRokY4ePSpJOnDggI4dO+ax5gAAAABfqNdO8jfffKNBgwZp7969Ki8v18CBAxUUFKQ5c+boxx9/1AsvvODpPgEAAACvqddO8uTJk9WtWzcVFxerWbNm9viIESO0adMmjzUHAAAA+EK9727x0UcfKSAgwG08KipK3377rUcaAwAAAHylXjvJNTU1qq6urjW+f/9+BQUFnXdTAAAAgC/VKyQPHDhQzzzzjP3Y4XDo2LFjeuSRR/iqagAAAFz06nW5xdNPP61+/fqpc+fO+vHHHzV69Gjt3LlTISEheumllzzdIwAAAOBV9QrJERERys3N1UsvvaRPPvlENTU1Gjt2rO644w63D/IBAAAAF6N6hWRJatasme655x7dc889nuwHAAAA8Ll6heTly5ef8fhdd91Vr2YAAACAhqBeIXny5MlujysrK/XDDz8oICBAzZs3JyQDAADgolavu1sUFxe7/Rw7dkw7duzQDTfcwAf3AAAAcNGrV0g+lQ4dOuiJJ56otcsMAAAAXGw8FpIlyc/PTwcOHPDklAAAAIDX1eua5DfeeMPtsWVZKigo0IIFC9SrVy+PNAYAAAD4Sr1C8vDhw90eOxwOtW7dWjfddJOeeuopT/QFAAAA+Ey9QnJNTY2n+wAAAAAaDI9ekwwAAAA0BvXaSZ46dWqda+fPn1+fUwAAAAA+U6+QvG3bNn3yySeqqqpSdHS0JOmrr76Sn5+frr32WrvO4XB4pksAAADAi+oVkhMTExUUFKRly5apVatWkn76gpG7775bN954o1JSUjzaJAAAAOBN9bom+amnntLs2bPtgCxJrVq10uOPP87dLQAAAHDRq1dILi0t1cGDB2uNFxUV6ejRo+fdFAAAAOBL9QrJI0aM0N13361//etf2r9/v/bv369//etfGjt2rEaOHOnpHgEAAACvqtc1yS+88IJSU1N15513qrKy8qeJ/P01duxYzZ0716MNAgAAAN5Wr5DcvHlzPf/885o7d66+/vprWZal9u3bKzAw0NP9AQAAAF53Xl8mUlBQoIKCAnXs2FGBgYGyLMtTfQEAAAA+U6+Q/P3336t///7q2LGjhgwZooKCAknSvffey+3fAAAAcNGrV0j+wx/+oCZNmmjv3r1q3ry5PX7rrbcqPT3dY80BAAAAvlCva5I3bNigt99+W5dddpnbeIcOHfTNN994pDEAAADAV+q1k1xWVua2g3zCd999J6fTed5NAQAAAL5Ur5Dcu3dvLV++3H7scDhUU1OjuXPnql+/fh5rDgAAAPCFel1uMXfuXPXt21dbt25VRUWFpk2bpu3bt+vw4cP66KOPPN0jAAAA4FX12knu3LmzPvvsM11//fUaOHCgysrKNHLkSG3btk1XXnmlp3sEAAAAvOqcd5IrKysVHx+vRYsWacaMGReiJwAAAMCnznknuUmTJsrLy5PD4bgQ/QAAAAA+V6/LLe666y4tXrzY070AAAAADUK9PrhXUVGhv//979q4caO6deumwMBAt+Pz58/3SHMAAACAL5xTSP7vf/+rtm3bKi8vT9dee60k6auvvnKr4TIMAAAAXOzOKSR36NBBBQUFeu+99yT99DXUzz77rMLCwi5IcwAAAIAvnNM1yZZluT1+6623VFZW5tGGAAAAAF+r1wf3TjBDMwAAANAYnFNIdjgcta455hpkAAAANDbndE2yZVlKTk6W0+mUJP3444/63e9+V+vuFq+++qrnOgQAAAC87JxC8pgxY9we33nnnR5tBgAAAGgIzikkL1my5EL1AQAAADQY5/XBPQAAAKAxIiQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGDwaUj+8MMPlZiYqIiICDkcDr322mtuxy3L0vTp0xUREaFmzZqpb9++2r59u1tNeXm5Jk6cqJCQEAUGBmrYsGHav3+/W01xcbGSkpLkcrnkcrmUlJSkI0eOuNXs3btXiYmJCgwMVEhIiCZNmqSKiooL8bIBAADQwPk0JJeVlelXv/qVFixYcMrjc+bM0fz587VgwQJ9/PHHCg8P18CBA3X06FG7ZsqUKVq7dq3S0tKUkZGhY8eOaejQoaqurrZrRo8erdzcXKWnpys9PV25ublKSkqyj1dXV+vmm29WWVmZMjIylJaWpjVr1iglJeXCvXgAAAA0WP6+PPngwYM1ePDgUx6zLEvPPPOMHnroIY0cOVKStGzZMoWFhWn16tW6//77VVJSosWLF2vFihUaMGCAJGnlypWKjIzUO++8o4SEBOXn5ys9PV3Z2dnq3r27JOnFF19UXFycduzYoejoaG3YsEFffPGF9u3bp4iICEnSU089peTkZM2cOVMtWrTwwrsBAACAhsKnIflMdu/ercLCQsXHx9tjTqdTffr0UWZmpu6//37l5OSosrLSrSYiIkIxMTHKzMxUQkKCsrKy5HK57IAsST169JDL5VJmZqaio6OVlZWlmJgYOyBLUkJCgsrLy5WTk6N+/fqdssfy8nKVl5fbj0tLSyVJVVVVqqqq8th7cSZ+qvHKeYDGzlu/s2g8WH8Bz/D2+lvX8zXYkFxYWChJCgsLcxsPCwvTN998Y9cEBASoVatWtWpOPL+wsFChoaG15g8NDXWrMc/TqlUrBQQE2DWnMnv2bM2YMaPWeFZWlgIDA8/2Ej2iV9APXjkP0NhlZGT4ugVcZFh/Ac/w9vpbVlZWp7oGG5JPcDgcbo8ty6o1ZjJrTlVfnxrTAw88oKlTp9qPS0tLFRkZqbi4OK9dorF6ZY5XzgM0dnfcEuvrFnCRYf0FPMPb6++Jv/yfTYMNyeHh4ZJ+2uVt06aNPV5UVGTv+oaHh6uiokLFxcVuu8lFRUXq2bOnXXPw4MFa8x86dMhtns2bN7sdLy4uVmVlZa0d5pM5nU45nc5a4/7+/vL3985bW81d/ACP8NbvLBoP1l/AM7y9/tb1fA32N7xdu3YKDw/Xxo0b7bGKigp98MEHdgCOjY1VkyZN3GoKCgqUl5dn18TFxamkpERbtmyxazZv3qySkhK3mry8PBUUFNg1GzZskNPpVGwsu0sAAAA/Nz7dOjl27Jh27dplP969e7dyc3MVHBysyy+/XFOmTNGsWbPUoUMHdejQQbNmzVLz5s01evRoSZLL5dLYsWOVkpKiSy+9VMHBwUpNTVWXLl3su1106tRJgwYN0rhx47Ro0SJJ0n333aehQ4cqOjpakhQfH6/OnTsrKSlJc+fO1eHDh5Wamqpx48ZxZwsAAICfIZ+G5K1bt7rdOeLE9b1jxozR0qVLNW3aNB0/flzjx49XcXGxunfvrg0bNigoKMh+ztNPPy1/f3+NGjVKx48fV//+/bV06VL5+fnZNatWrdKkSZPsu2AMGzbM7d7Mfn5+WrduncaPH69evXqpWbNmGj16tObNm3eh3wIAAAA0QA7LsixfN9FYlJaWyuVyqaSkxGs70GOXfuyV8wCN3eLk63zdAi4yrL+AZ3h7/a1rXmuw1yQDAAAAvkJIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBgAAAAyEZAAAAMDQoEPy9OnT5XA43H7Cw8Pt45Zlafr06YqIiFCzZs3Ut29fbd++3W2O8vJyTZw4USEhIQoMDNSwYcO0f/9+t5ri4mIlJSXJ5XLJ5XIpKSlJR44c8cZLBAAAQAPUoEOyJF199dUqKCiwfz7//HP72Jw5czR//nwtWLBAH3/8scLDwzVw4EAdPXrUrpkyZYrWrl2rtLQ0ZWRk6NixYxo6dKiqq6vtmtGjRys3N1fp6elKT09Xbm6ukpKSvPo6AQAA0HD4+7qBs/H393fbPT7Bsiw988wzeuihhzRy5EhJ0rJlyxQWFqbVq1fr/vvvV0lJiRYvXqwVK1ZowIABkqSVK1cqMjJS77zzjhISEpSfn6/09HRlZ2ere/fukqQXX3xRcXFx2rFjh6Kjo733YgEAANAgNPiQvHPnTkVERMjpdKp79+6aNWuWrrjiCu3evVuFhYWKj4+3a51Op/r06aPMzEzdf//9ysnJUWVlpVtNRESEYmJilJmZqYSEBGVlZcnlctkBWZJ69Oghl8ulzMzMM4bk8vJylZeX249LS0slSVVVVaqqqvLk23BafqrxynmAxs5bv7NoPFh/Ac/w9vpb1/M16JDcvXt3LV++XB07dtTBgwf1+OOPq2fPntq+fbsKCwslSWFhYW7PCQsL0zfffCNJKiwsVEBAgFq1alWr5sTzCwsLFRoaWuvcoaGhds3pzJ49WzNmzKg1npWVpcDAwLq/0PPQK+gHr5wHaOwyMjJ83QIuMqy/gGd4e/0tKyurU12DDsmDBw+2/92lSxfFxcXpyiuv1LJly9SjRw9JksPhcHuOZVm1xkxmzanq6zLPAw88oKlTp9qPS0tLFRkZqbi4OLVo0eKMz/WU1StzvHIeoLG745ZYX7eAiwzrL+AZ3l5/T/zl/2wadEg2BQYGqkuXLtq5c6eGDx8u6aed4DZt2tg1RUVF9u5yeHi4KioqVFxc7LabXFRUpJ49e9o1Bw8erHWuQ4cO1dqlNjmdTjmdzlrj/v7+8vf3zltb3fA/ewlcFLz1O4vGg/UX8Axvr791Pd9F9RteXl6u/Px8tWnTRu3atVN4eLg2btxoH6+oqNAHH3xgB+DY2Fg1adLEraagoEB5eXl2TVxcnEpKSrRlyxa7ZvPmzSopKbFrAAAA8PPSoLdOUlNTlZiYqMsvv1xFRUV6/PHHVVpaqjFjxsjhcGjKlCmaNWuWOnTooA4dOmjWrFlq3ry5Ro8eLUlyuVwaO3asUlJSdOmllyo4OFipqanq0qWLfbeLTp06adCgQRo3bpwWLVokSbrvvvs0dOhQ7mwBAADwM9WgQ/L+/ft1++2367vvvlPr1q3Vo0cPZWdnKyoqSpI0bdo0HT9+XOPHj1dxcbG6d++uDRs2KCgoyJ7j6aeflr+/v0aNGqXjx4+rf//+Wrp0qfz8/OyaVatWadKkSfZdMIYNG6YFCxZ498UCAACgwXBYlmX5uonGorS0VC6XSyUlJV774N7YpR975TxAY7c4+Tpft4CLDOsv4BneXn/rmtcuqmuSAQAAAG8gJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQDAAAABkIyAAAAYCAkAwAAAAZCMgAAAGAgJAMAAAAGQjIAAABgICQbnn/+ebVr105NmzZVbGys/vOf//i6JQAAAHgZIfkkL7/8sqZMmaKHHnpI27Zt04033qjBgwdr7969vm4NAAAAXkRIPsn8+fM1duxY3XvvverUqZOeeeYZRUZGauHChb5uDQAAAF7k7+sGGoqKigrl5OToz3/+s9t4fHy8MjMzT/mc8vJylZeX249LSkokSYcPH1ZVVdWFa/Yk1cePeuU8QGN3+PBhX7eAiwzrL+AZ3l5/S0tLJUmWZZ2xjpD8f7777jtVV1crLCzMbTwsLEyFhYWnfM7s2bM1Y8aMWuPt2rW7ID0CuHCWjfd1BwDw8+Sr9ffo0aNyuVynPU5INjgcDrfHlmXVGjvhgQce0NSpU+3HNTU1Onz4sC699NLTPgc/P6WlpYqMjNS+ffvUokULX7cDAD8brL84FcuydPToUUVERJyxjpD8f0JCQuTn51dr17ioqKjW7vIJTqdTTqfTbaxly5YXqkVc5Fq0aMEiDQA+wPoL05l2kE/gg3v/JyAgQLGxsdq4caPb+MaNG9WzZ08fdQUAAABfYCf5JFOnTlVSUpK6deumuLg4/e1vf9PevXv1u9/9ztetAQAAwIsIySe59dZb9f333+vRRx9VQUGBYmJitH79ekVFRfm6NVzEnE6nHnnkkVqX5gAALizWX5wPh3W2+18AAAAAPzNckwwAAAAYCMkAAACAgZAMAAAAGAjJwFm8//77cjgcOnLkiK9bOScOh0OvvfbaGWu+//57hYaGas+ePXWe980331TXrl1VU1Nzfg0C+Nmoy3rU0PTt21dTpkw5a13v3r21evXqOs9bVFSk1q1b69tvvz2P7uANhGQ0GMnJyXI4HHriiSfcxl977bVz/gbDtm3b6plnnqlT7bZt2/Tb3/5WYWFhatq0qTp27Khx48bpq6++OqdzXoxmz56txMREtW3b1h7bu3evEhMTFRgYqJCQEE2aNEkVFRX28aFDh8rhcJzTfxQANF6FhYWaOHGirrjiCjmdTkVGRioxMVGbNm3ydWsX3JtvvqnCwkLddttt9tjf/vY39e3bVy1atDjlBktoaKiSkpL0yCOPeLlbnCtCMhqUpk2b6sknn1RxcbFXzvfmm2+qR48eKi8v16pVq5Sfn68VK1bI5XLp4YcfvqDnPjl4+sLx48e1ePFi3XvvvfZYdXW1br75ZpWVlSkjI0NpaWlas2aNUlJS3J57991367nnnvN2ywAamD179ig2Nlbvvvuu5syZo88//1zp6enq16+fJkyYcEHPXVlZeUHnr4tnn31Wd999ty655P+PUz/88IMGDRqkBx988LTPu/vuu7Vq1Sqv/bcO9WQBDcSYMWOsoUOHWldddZX1xz/+0R5fu3atZf5f9V//+pfVuXNnKyAgwIqKirLmzZtnH+vTp48lye3nVMrKyqyQkBBr+PDhpzxeXFxsWZZlvffee5Yk65133rFiY2OtZs2aWXFxcdaXX37p1vstt9zi9vzJkydbffr0cetrwoQJ1h/+8Afr0ksvtXr37l2nuS3Lst544w3r2muvtZxOp9WuXTtr+vTpVmVlpX38q6++sm688UbL6XRanTp1sjZs2GBJstauXXvK12ZZlrVmzRorJCTEbWz9+vXWJZdcYn377bf22EsvvWQ5nU6rpKTEHtuzZ48lyfr6669POz+Axm/w4MHWL3/5S+vYsWO1jp1YQy3LsiRZL774ojV8+HCrWbNmVvv27a3XX3/dPr5kyRLL5XK5Pd9c+x955BHrV7/6lbV48WKrXbt2lsPhsGpqas46t2VZ1vbt263BgwdbgYGBVmhoqHXnnXdahw4dso8fO3bMSkpKsgIDA63w8HBr3rx5Vp8+fazJkyef9rUfOnTIcjgcVl5e3imPn1jfT34fTta2bVtr8eLFp50fvsdOMhoUPz8/zZo1S88995z2799/ypqcnByNGjVKt912mz7//HNNnz5dDz/8sJYuXSpJevXVV3XZZZfZXwpTUFBwynnefvttfffdd5o2bdopj7ds2dLt8UMPPaSnnnpKW7dulb+/v+65555zfn3Lli2Tv7+/PvroIy1atKhOc7/99tu68847NWnSJH3xxRdatGiRli5dqpkzZ0qSampqNHLkSPn5+Sk7O1svvPCC/vSnP521lw8//FDdunVzG8vKylJMTIwiIiLssYSEBJWXlysnJ8cei4qKUmhoqP7zn/+c83sAoHE4fPiw0tPTNWHCBAUGBtY6bq6hM2bM0KhRo/TZZ59pyJAhuuOOO3T48OFzOueuXbv0yiuvaM2aNcrNza3T3AUFBerTp49+/etfa+vWrUpPT9fBgwc1atQo+/l//OMf9d5772nt2rXasGGD3n//fbc171QyMjLUvHlzderU6ZxewwnXX389a2gDxzfuocEZMWKEfv3rX+uRRx7R4sWLax2fP3+++vfvb18O0bFjR33xxReaO3eukpOTFRwcLD8/PwUFBSk8PPy059m5c6ck6aqrrqpTXzNnzlSfPn0kSX/+8591880368cff1TTpk3r/Nrat2+vOXPm2I8LCwvPOvfMmTP15z//WWPGjJEkXXHFFXrsscc0bdo0PfLII3rnnXeUn5+vPXv26LLLLpMkzZo1S4MHDz5jL3v27HELwyf6CQsLcxtr1aqVAgIC7F5P+OUvf3lOH/gD0Ljs2rVLlmXVeQ1NTk7W7bffLkn2ZsiWLVs0aNCgOp+zoqJCK1asUOvWres898KFC3Xttddq1qxZdv0//vEPRUZG6quvvlJERIQWL16s5cuXa+DAgZJ+2tA4sZ6ezp49exQWFuZ2qcW5+OUvf6lt27bV67nwDnaS0SA9+eSTWrZsmb744otax/Lz89WrVy+3sV69emnnzp2qrq6u8zmsc/yyyWuuucb+d5s2bST99Cnlc2Hu3NZl7pycHD366KP6xS9+Yf+MGzdOBQUF+uGHH5Sfn6/LL7/cbUGPi4s7ay/Hjx8/ZcA/1YckLcuqNd6sWTP98MMPZz0PgMbpxBpa1w9Wn7zOBQYGKigo6JzX0KioqFoB+Wxz5+Tk6L333nNbQ08E+6+//lpff/21Kioq3NbN4OBgRUdHn7GX062hdcUa2vARktEg9e7dWwkJCaf84MOpAtu5Bl7ppx1oSfryyy/rVN+kSRP73yfOf+I2aJdcckmtHk71oZJT/UnybHPX1NRoxowZys3NtX8+//xz7dy5U02bNj3la6/Lf7RCQkJqfWgkPDy81o5xcXGxKisra+0wHz58+JT/sQLw89ChQwc5HA7l5+fXqf7kdU76aZ26EGuoOXdNTY0SExPd1tDc3Fzt3LlTvXv3rtd/P6RTr6HngjW04SMko8F64okn9O9//1uZmZlu4507d1ZGRobbWGZmpjp27Cg/Pz9JUkBAwFl3lePj4xUSEuJ2+cPJzuW+yK1bt6517fPJ18udj2uvvVY7duxQ+/bta/1ccskl6ty5s/bu3asDBw7Yz8nKyjrrvF27dq21Ux8XF6e8vDy317JhwwY5nU7FxsbaYz/++KO+/vprde3a1QOvEMDFKDg4WAkJCfrrX/+qsrKyWsfPdQ09evSo2zyeXEO3b9+utm3b1lpDAwMD1b59ezVp0kTZ2dn2c4qLi896G9CuXbuqsLCw3kE5Ly+PNbSBIySjwerSpYvuuOOOWrcaS0lJ0aZNm/TYY4/pq6++0rJly7RgwQKlpqbaNW3bttWHH36ob7/9Vt99990p5w8MDNTf//53rVu3TsOGDdM777yjPXv2aOvWrZo2bZp+97vf1bnXm266SVu3btXy5cu1c+dOPfLII8rLy6vfCzf85S9/0fLlyzV9+nRt375d+fn5evnll/X//t//kyQNGDBA0dHRuuuuu/Tpp5/qP//5jx566KGzzpuQkKDt27e7LfDx8fHq3LmzkpKStG3bNm3atEmpqakaN26cWrRoYddlZ2fL6XTW6bIOAI3X888/r+rqal1//fVas2aNdu7cqfz8fD377LPntD50795dzZs314MPPqhdu3Zp9erV9oexz9eECRN0+PBh3X777dqyZYv++9//asOGDbrnnntUXV2tX/ziFxo7dqz++Mc/atOmTcrLy1NycvJZrzXu2rWrWrdurY8++shtvLCwULm5udq1a5ck6fPPP1dubq7bhxR/+OEH5eTkKD4+3iOvERcGIRkN2mOPPVbrT2HXXnutXnnlFaWlpSkmJkZ/+ctf9Oijjyo5OdmuefTRR7Vnzx5deeWVZ/xz1i233KLMzEw1adJEo0eP1lVXXaXbb79dJSUlevzxx+vcZ0JCgh5++GFNmzZN1113nY4ePaq77rrrnF/v6eZ+8803tXHjRl133XXq0aOH5s+fr6ioKEk//Zly7dq1Ki8v1/XXX697773XvvPFmXTp0kXdunXTK6+8Yo/5+flp3bp1atq0qXr16qVRo0Zp+PDhmjdvnttzX3rpJd1xxx1q3ry5R14jgItTu3bt9Mknn6hfv35KSUlRTEyMBg4cqE2bNmnhwoV1nic4OFgrV67U+vXr1aVLF7300kuaPn26R3qMiIjQRx99pOrqaiUkJCgmJkaTJ0+Wy+Wyg/DcuXPVu3dvDRs2TAMGDNANN9zg9tezU/Hz89M999yjVatWuY2/8MIL6tq1q8aNGyfpp8sHu3btqjfeeMOuef3113X55Zfrxhtv9MhrxIXhsOp7MQ6Ai9769euVmpqqvLy8On9C+9ChQ7rqqqu0detWtWvX7gJ3CAAN18GDB3X11VcrJyfH3rioi+uvv15TpkzR6NGjL2B3OF/cAg74GRsyZIh27typb7/9VpGRkXV6zu7du/X8888TkAH87IWFhWnx4sXau3dvnUNyUVGRfvOb39i3rEPDxU4yAAAAYOCaZAAAAMBASAYAAAAMhGQAAADAQEgGAAAADIRkAAAAwEBIBoCfKYfDoddee83XbQBAg0RIBoBGqrCwUBMnTtQVV1whp9OpyMhIJSYmatOmTb5uDQAaPL5MBAAaoT179qhXr15q2bKl5syZo2uuuUaVlZV6++23NWHCBH355ZcX5LyVlZVq0qTJBZkbALyJnWQAaITGjx8vh8OhLVu26De/+Y06duyoq6++WlOnTlV2drZd991332nEiBFq3ry5OnTooDfeeMM+tnTpUrVs2dJt3tdee00Oh8N+PH36dP3617/WP/7xD3vH2rIsORwO/f3vfz/t3ADQ0BGSAaCROXz4sNLT0zVhwgQFBgbWOn5y8J0xY4ZGjRqlzz77TEOGDNEdd9yhw4cPn9P5du3apVdeeUVr1qxRbm6uR+cGAF8hJANAI7Nr1y5ZlqWrrrrqrLXJycm6/fbb1b59e82aNUtlZWXasmXLOZ2voqJCK1asUNeuXXXNNdfYO82emBsAfIWQDACNjGVZkuR2WcTpXHPNNfa/AwMDFRQUpKKionM6X1RUlFq3bn1B5gYAXyEkA0Aj06FDBzkcDuXn55+11vyQncPhUE1NjSTpkksusQP3CZWVlbXmONUlHWebGwAaOkIyADQywcHBSkhI0F//+leVlZXVOn7kyJE6zdO6dWsdPXrUbY6TrzkGgMaMkAwAjdDzzz+v6upqXX/99VqzZo127typ/Px8Pfvss4qLi6vTHN27d1fz5s314IMPateuXVq9erWWLl16YRsHgAaCkAwAjVC7du30ySefqF+/fkpJSVFMTIwGDhyoTZs2aeHChXWaIzg4WCtXrtT69evVpUsXvfTSS5o+ffqFbRwAGgiHZV5wBgAAAPzMsZMMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACAgZAMAAAAGAjJAAAAgIGQDAAAABgIyQAAAICBkAwAAAAYCMkAAACA4f8DqIH5fscpHU4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 오버 샘플링 (SMOTE)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
        "\n",
        "churn_counts_resampled = y_train_resampled.value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(bars, churn_counts_resampled, alpha=0.7, width=0.6)\n",
        "plt.xticks(bars, labels)\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcMDTATiOaxf"
      },
      "source": [
        "### 언더 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l60YLNmOda3",
        "outputId": "3f55307d-4cd9-4df8-bed1-c8d047b4148f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n",
            "Accuracy: 0.5806, AUC: 0.6113, F1 Score: 0.4374\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.58      0.67      6453\n",
            "           1       0.35      0.57      0.44      2558\n",
            "\n",
            "    accuracy                           0.58      9011\n",
            "   macro avg       0.56      0.58      0.55      9011\n",
            "weighted avg       0.66      0.58      0.60      9011\n",
            "\n",
            "============================================================\n",
            "Model: KNN\n",
            "Accuracy: 0.5473, AUC: 0.5558, F1 Score: 0.4013\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.55      0.64      6453\n",
            "           1       0.32      0.53      0.40      2558\n",
            "\n",
            "    accuracy                           0.55      9011\n",
            "   macro avg       0.54      0.54      0.52      9011\n",
            "weighted avg       0.63      0.55      0.57      9011\n",
            "\n",
            "============================================================\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.5355, AUC: 0.5353, F1 Score: 0.3953\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.54      0.62      6453\n",
            "           1       0.31      0.53      0.40      2558\n",
            "\n",
            "    accuracy                           0.54      9011\n",
            "   macro avg       0.53      0.54      0.51      9011\n",
            "weighted avg       0.62      0.54      0.56      9011\n",
            "\n",
            "============================================================\n",
            "Model: Random Forest\n",
            "Accuracy: 0.5939, AUC: 0.6410, F1 Score: 0.4620\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.59      0.67      6453\n",
            "           1       0.37      0.61      0.46      2558\n",
            "\n",
            "    accuracy                           0.59      9011\n",
            "   macro avg       0.58      0.60      0.57      9011\n",
            "weighted avg       0.67      0.59      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.5852, AUC: 0.6641, F1 Score: 0.4853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.54      0.65      6453\n",
            "           1       0.37      0.69      0.49      2558\n",
            "\n",
            "    accuracy                           0.59      9011\n",
            "   macro avg       0.59      0.62      0.57      9011\n",
            "weighted avg       0.69      0.59      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: XGBoost\n",
            "Accuracy: 0.5945, AUC: 0.6492, F1 Score: 0.4701\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.58      0.67      6453\n",
            "           1       0.37      0.63      0.47      2558\n",
            "\n",
            "    accuracy                           0.59      9011\n",
            "   macro avg       0.59      0.61      0.57      9011\n",
            "weighted avg       0.68      0.59      0.61      9011\n",
            "\n",
            "============================================================\n",
            "[LightGBM] [Info] Number of positive: 10394, number of negative: 10394\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3026\n",
            "[LightGBM] [Info] Number of data points in the train set: 20788, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Model: LightGBM\n",
            "Accuracy: 0.5918, AUC: 0.6609, F1 Score: 0.4773\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.57      0.67      6453\n",
            "           1       0.37      0.66      0.48      2558\n",
            "\n",
            "    accuracy                           0.59      9011\n",
            "   macro avg       0.59      0.61      0.57      9011\n",
            "weighted avg       0.68      0.59      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: CatBoost\n",
            "Accuracy: 0.6016, AUC: 0.6660, F1 Score: 0.4861\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.58      0.67      6453\n",
            "           1       0.38      0.66      0.49      2558\n",
            "\n",
            "    accuracy                           0.60      9011\n",
            "   macro avg       0.60      0.62      0.58      9011\n",
            "weighted avg       0.69      0.60      0.62      9011\n",
            "\n",
            "============================================================\n",
            "Model: MLP\n",
            "Accuracy: 0.4259, AUC: 0.5893, F1 Score: 0.4523\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.26      0.40      6453\n",
            "           1       0.31      0.84      0.45      2558\n",
            "\n",
            "    accuracy                           0.43      9011\n",
            "   macro avg       0.56      0.55      0.42      9011\n",
            "weighted avg       0.66      0.43      0.41      9011\n",
            "\n",
            "============================================================\n",
            "                 Model  Accuracy       AUC  F1 Score\n",
            "7             CatBoost  0.601598  0.666017  0.486115\n",
            "4    Gradient Boosting  0.585174  0.664127  0.485266\n",
            "6             LightGBM  0.591832  0.660910  0.477260\n",
            "5              XGBoost  0.594496  0.649184  0.470128\n",
            "3        Random Forest  0.593941  0.641040  0.461991\n",
            "8                  MLP  0.425924  0.589333  0.452303\n",
            "0  Logistic Regression  0.580624  0.611283  0.437398\n",
            "1                  KNN  0.547331  0.555763  0.401292\n",
            "2        Decision Tree  0.535457  0.535256  0.395261\n"
          ]
        }
      ],
      "source": [
        "# 모델 리스트\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# 언더샘플링을 적용하기 위한 준비\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# 학습 및 테스트 데이터에서 언더샘플링 적용\n",
        "x_train_resampled, y_train_resampled = undersampler.fit_resample(x_train, y_train)\n",
        "\n",
        "# 모델 학습 및 평가 결과 저장용 리스트 생성\n",
        "results = []\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "for name, model in models.items():\n",
        "    # 언더샘플링된 데이터를 사용해 모델 학습\n",
        "    model.fit(x_train_resampled, y_train_resampled)\n",
        "\n",
        "    # 테스트 데이터에 대한 예측\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_proba = model.predict_proba(x_test)[:, 1] if hasattr(model, \"predict_proba\") else np.zeros(len(y_test))\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': name, 'Accuracy': accuracy, 'AUC': auc, 'F1 Score': f1})\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, F1 Score: {f1:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 결과 리스트를 데이터프레임으로 변환하여 성능 확인\n",
        "resultsUn = pd.DataFrame(results)\n",
        "resultsUn.sort_values(by='F1 Score', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Model Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.601598</td>\n",
              "      <td>0.666017</td>\n",
              "      <td>0.486115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.585174</td>\n",
              "      <td>0.664127</td>\n",
              "      <td>0.485266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.591832</td>\n",
              "      <td>0.660910</td>\n",
              "      <td>0.477260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.594496</td>\n",
              "      <td>0.649184</td>\n",
              "      <td>0.470128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.593941</td>\n",
              "      <td>0.641040</td>\n",
              "      <td>0.461991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.425924</td>\n",
              "      <td>0.589333</td>\n",
              "      <td>0.452303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.580624</td>\n",
              "      <td>0.611283</td>\n",
              "      <td>0.437398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.547331</td>\n",
              "      <td>0.555763</td>\n",
              "      <td>0.401292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.535457</td>\n",
              "      <td>0.535256</td>\n",
              "      <td>0.395261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy       AUC  F1 Score\n",
              "7             CatBoost  0.601598  0.666017  0.486115\n",
              "4    Gradient Boosting  0.585174  0.664127  0.485266\n",
              "6             LightGBM  0.591832  0.660910  0.477260\n",
              "5              XGBoost  0.594496  0.649184  0.470128\n",
              "3        Random Forest  0.593941  0.641040  0.461991\n",
              "8                  MLP  0.425924  0.589333  0.452303\n",
              "0  Logistic Regression  0.580624  0.611283  0.437398\n",
              "1                  KNN  0.547331  0.555763  0.401292\n",
              "2        Decision Tree  0.535457  0.535256  0.395261"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 최종 결과 출력\n",
        "print(\"\\nFinal Model Results:\")\n",
        "resultsUn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9oRMHkJMY56"
      },
      "source": [
        "### 오버 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eSWpQ39MY56",
        "outputId": "9bbb704a-21ba-4f8a-aa55-002029d7f11c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n",
            "Accuracy: 0.7151, AUC: 0.6048, F1 Score: 0.0939\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.98      0.83      6453\n",
            "           1       0.48      0.05      0.09      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.60      0.51      0.46      9011\n",
            "weighted avg       0.65      0.72      0.62      9011\n",
            "\n",
            "============================================================\n",
            "Model: KNN\n",
            "Accuracy: 0.5442, AUC: 0.5474, F1 Score: 0.3886\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.56      0.64      6453\n",
            "           1       0.31      0.51      0.39      2558\n",
            "\n",
            "    accuracy                           0.54      9011\n",
            "   macro avg       0.53      0.53      0.51      9011\n",
            "weighted avg       0.62      0.54      0.57      9011\n",
            "\n",
            "============================================================\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.6053, AUC: 0.5331, F1 Score: 0.3451\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.70      0.72      6453\n",
            "           1       0.33      0.37      0.35      2558\n",
            "\n",
            "    accuracy                           0.61      9011\n",
            "   macro avg       0.53      0.53      0.53      9011\n",
            "weighted avg       0.62      0.61      0.61      9011\n",
            "\n",
            "============================================================\n",
            "Model: Random Forest\n",
            "Accuracy: 0.7092, AUC: 0.6294, F1 Score: 0.1833\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82      6453\n",
            "           1       0.45      0.11      0.18      2558\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.59      0.53      0.50      9011\n",
            "weighted avg       0.65      0.71      0.64      9011\n",
            "\n",
            "============================================================\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.7166, AUC: 0.6442, F1 Score: 0.0706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.99      0.83      6453\n",
            "           1       0.51      0.04      0.07      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.62      0.51      0.45      9011\n",
            "weighted avg       0.66      0.72      0.62      9011\n",
            "\n",
            "============================================================\n",
            "Model: XGBoost\n",
            "Accuracy: 0.7136, AUC: 0.6472, F1 Score: 0.2624\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.93      0.82      6453\n",
            "           1       0.49      0.18      0.26      2558\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.61      0.55      0.54      9011\n",
            "weighted avg       0.67      0.71      0.66      9011\n",
            "\n",
            "============================================================\n",
            "[LightGBM] [Info] Number of positive: 25646, number of negative: 25646\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4502\n",
            "[LightGBM] [Info] Number of data points in the train set: 51292, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Model: LightGBM\n",
            "Accuracy: 0.7189, AUC: 0.6620, F1 Score: 0.1316\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.97      0.83      6453\n",
            "           1       0.53      0.08      0.13      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.63      0.52      0.48      9011\n",
            "weighted avg       0.67      0.72      0.63      9011\n",
            "\n",
            "============================================================\n",
            "Model: CatBoost\n",
            "Accuracy: 0.7242, AUC: 0.6653, F1 Score: 0.2208\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.96      0.83      6453\n",
            "           1       0.56      0.14      0.22      2558\n",
            "\n",
            "    accuracy                           0.72      9011\n",
            "   macro avg       0.65      0.55      0.53      9011\n",
            "weighted avg       0.69      0.72      0.66      9011\n",
            "\n",
            "============================================================\n",
            "Model: MLP\n",
            "Accuracy: 0.7145, AUC: 0.6097, F1 Score: 0.1251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.97      0.83      6453\n",
            "           1       0.48      0.07      0.13      2558\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.60      0.52      0.48      9011\n",
            "weighted avg       0.66      0.71      0.63      9011\n",
            "\n",
            "============================================================\n",
            "\n",
            "Final Model Results:\n",
            "                 Model  Accuracy       AUC  F1 Score\n",
            "1                  KNN  0.544224  0.547411  0.388566\n",
            "2        Decision Tree  0.605260  0.533143  0.345056\n",
            "5              XGBoost  0.713572  0.647152  0.262361\n",
            "7             CatBoost  0.724226  0.665325  0.220759\n",
            "3        Random Forest  0.709244  0.629429  0.183292\n",
            "6             LightGBM  0.718899  0.662008  0.131642\n",
            "8                  MLP  0.714460  0.609656  0.125128\n",
            "0  Logistic Regression  0.715126  0.604807  0.093893\n",
            "4    Gradient Boosting  0.716569  0.644170  0.070597\n"
          ]
        }
      ],
      "source": [
        "# 오버샘플링 적용 (train 데이터에만)\n",
        "smote = SMOTE(random_state=42)\n",
        "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
        "\n",
        "# 모델 리스트\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train_resampled, y_train_resampled)\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_proba = model.predict_proba(x_test)[:, 1] if hasattr(model, \"predict_proba\") else np.zeros(len(y_test))\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': name, 'Accuracy': accuracy, 'AUC': auc, 'F1 Score': f1})\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, F1 Score: {f1:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 결과 리스트를 데이터프레임으로 변환하여 성능 확인\n",
        "results_SM = pd.DataFrame(results)\n",
        "results_SM.sort_values(by='F1 Score', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Model Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.544224</td>\n",
              "      <td>0.547411</td>\n",
              "      <td>0.388566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.605260</td>\n",
              "      <td>0.533143</td>\n",
              "      <td>0.345056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.713572</td>\n",
              "      <td>0.647152</td>\n",
              "      <td>0.262361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.724226</td>\n",
              "      <td>0.665325</td>\n",
              "      <td>0.220759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.709244</td>\n",
              "      <td>0.629429</td>\n",
              "      <td>0.183292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.718899</td>\n",
              "      <td>0.662008</td>\n",
              "      <td>0.131642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.714460</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.125128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.715126</td>\n",
              "      <td>0.604807</td>\n",
              "      <td>0.093893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.716569</td>\n",
              "      <td>0.644170</td>\n",
              "      <td>0.070597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy       AUC  F1 Score\n",
              "1                  KNN  0.544224  0.547411  0.388566\n",
              "2        Decision Tree  0.605260  0.533143  0.345056\n",
              "5              XGBoost  0.713572  0.647152  0.262361\n",
              "7             CatBoost  0.724226  0.665325  0.220759\n",
              "3        Random Forest  0.709244  0.629429  0.183292\n",
              "6             LightGBM  0.718899  0.662008  0.131642\n",
              "8                  MLP  0.714460  0.609656  0.125128\n",
              "0  Logistic Regression  0.715126  0.604807  0.093893\n",
              "4    Gradient Boosting  0.716569  0.644170  0.070597"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 최종 결과 출력\n",
        "print(\"\\nFinal Model Results:\")\n",
        "results_SM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pt9MJxESX3n"
      },
      "source": [
        "### LSTM, GRU, CNN, Transformer 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcftq36BQ2kv",
        "outputId": "aba42a22-5326-4ddd-f475-1460a185cad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LSTM + Attention ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6022 - loss: 0.6641 - val_accuracy: 0.1141 - val_loss: 0.9657\n",
            "Epoch 2/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 0.6410 - val_accuracy: 0.1476 - val_loss: 0.9018\n",
            "Epoch 3/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6387 - loss: 0.6353 - val_accuracy: 0.1457 - val_loss: 0.9775\n",
            "Epoch 4/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6380 - loss: 0.6353 - val_accuracy: 0.1742 - val_loss: 0.9452\n",
            "Epoch 5/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6385 - loss: 0.6333 - val_accuracy: 0.1662 - val_loss: 0.9615\n",
            "Epoch 6/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6501 - loss: 0.6251 - val_accuracy: 0.2854 - val_loss: 0.9018\n",
            "Epoch 7/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6514 - loss: 0.6272 - val_accuracy: 0.3303 - val_loss: 0.8656\n",
            "Epoch 8/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6460 - loss: 0.6281 - val_accuracy: 0.1875 - val_loss: 0.9912\n",
            "Epoch 9/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6579 - loss: 0.6150 - val_accuracy: 0.1575 - val_loss: 1.0040\n",
            "Epoch 10/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6567 - loss: 0.6196 - val_accuracy: 0.2391 - val_loss: 0.9210\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6913 - loss: 0.6037\n",
            "\u001b[1m 17/282\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Model: LSTM + Attention\n",
            "Accuracy: 0.6866, ROC-AUC: 0.6199, F1 Score: 0.6568\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.87      0.80      6420\n",
            "           1       0.42      0.24      0.31      2591\n",
            "\n",
            "    accuracy                           0.69      9011\n",
            "   macro avg       0.58      0.55      0.55      9011\n",
            "weighted avg       0.65      0.69      0.66      9011\n",
            "\n",
            "============================================================\n",
            "\n",
            "=== BiLSTM + CNN ===\n",
            "Epoch 1/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.6208 - loss: 0.6551 - val_accuracy: 0.0396 - val_loss: 0.9946\n",
            "Epoch 2/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6255 - loss: 0.6433 - val_accuracy: 0.1421 - val_loss: 0.9615\n",
            "Epoch 3/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6289 - loss: 0.6388 - val_accuracy: 0.1312 - val_loss: 0.9653\n",
            "Epoch 4/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6405 - loss: 0.6328 - val_accuracy: 0.2343 - val_loss: 0.8988\n",
            "Epoch 5/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6316 - loss: 0.6360 - val_accuracy: 0.1144 - val_loss: 1.0436\n",
            "Epoch 6/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6494 - loss: 0.6220 - val_accuracy: 0.3407 - val_loss: 0.8903\n",
            "Epoch 7/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6557 - loss: 0.6160 - val_accuracy: 0.3199 - val_loss: 0.8526\n",
            "Epoch 8/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6599 - loss: 0.6134 - val_accuracy: 0.2654 - val_loss: 0.9575\n",
            "Epoch 9/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6610 - loss: 0.6052 - val_accuracy: 0.1995 - val_loss: 1.0341\n",
            "Epoch 10/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6684 - loss: 0.5987 - val_accuracy: 0.2036 - val_loss: 1.0450\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6935 - loss: 0.6012\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Model: BiLSTM + CNN\n",
            "Accuracy: 0.6947, ROC-AUC: 0.6213, F1 Score: 0.6587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.89      0.81      6420\n",
            "           1       0.44      0.22      0.30      2591\n",
            "\n",
            "    accuracy                           0.69      9011\n",
            "   macro avg       0.59      0.55      0.55      9011\n",
            "weighted avg       0.65      0.69      0.66      9011\n",
            "\n",
            "============================================================\n",
            "\n",
            "=== GRU + Attention ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6186 - loss: 0.6609 - val_accuracy: 0.1390 - val_loss: 0.9727\n",
            "Epoch 2/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6255 - loss: 0.6485 - val_accuracy: 0.1455 - val_loss: 0.9298\n",
            "Epoch 3/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6364 - loss: 0.6380 - val_accuracy: 0.1563 - val_loss: 0.9451\n",
            "Epoch 4/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6293 - loss: 0.6401 - val_accuracy: 0.1397 - val_loss: 0.9781\n",
            "Epoch 5/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6433 - loss: 0.6311 - val_accuracy: 0.1551 - val_loss: 0.9566\n",
            "Epoch 6/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6388 - loss: 0.6313 - val_accuracy: 0.1812 - val_loss: 0.9714\n",
            "Epoch 7/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 0.6241 - val_accuracy: 0.1626 - val_loss: 0.9544\n",
            "Epoch 8/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6464 - loss: 0.6237 - val_accuracy: 0.1978 - val_loss: 0.9453\n",
            "Epoch 9/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6463 - loss: 0.6232 - val_accuracy: 0.2403 - val_loss: 0.9425\n",
            "Epoch 10/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6589 - loss: 0.6148 - val_accuracy: 0.1303 - val_loss: 1.0122\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7035 - loss: 0.5918\n",
            "\u001b[1m 66/282\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Model: GRU + Attention\n",
            "Accuracy: 0.7025, ROC-AUC: 0.6195, F1 Score: 0.6421\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.93      0.82      6420\n",
            "           1       0.44      0.14      0.21      2591\n",
            "\n",
            "    accuracy                           0.70      9011\n",
            "   macro avg       0.59      0.53      0.51      9011\n",
            "weighted avg       0.65      0.70      0.64      9011\n",
            "\n",
            "============================================================\n",
            "\n",
            "=== Transformer ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6065 - loss: 0.6680 - val_accuracy: 0.0326 - val_loss: 0.8831\n",
            "Epoch 2/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6267 - loss: 0.6507 - val_accuracy: 0.1723 - val_loss: 0.9637\n",
            "Epoch 3/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6219 - loss: 0.6459 - val_accuracy: 0.2285 - val_loss: 0.8659\n",
            "Epoch 4/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6356 - loss: 0.6409 - val_accuracy: 0.0290 - val_loss: 0.9351\n",
            "Epoch 5/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6291 - loss: 0.6456 - val_accuracy: 0.0372 - val_loss: 1.0858\n",
            "Epoch 6/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6385 - loss: 0.6386 - val_accuracy: 0.2393 - val_loss: 0.9399\n",
            "Epoch 7/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6273 - loss: 0.6412 - val_accuracy: 0.0729 - val_loss: 0.9558\n",
            "Epoch 8/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6367 - loss: 0.6379 - val_accuracy: 0.0676 - val_loss: 1.0317\n",
            "Epoch 9/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6328 - loss: 0.6403 - val_accuracy: 0.1735 - val_loss: 0.9138\n",
            "Epoch 10/10\n",
            "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6388 - loss: 0.6331 - val_accuracy: 0.0567 - val_loss: 0.9679\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7158 - loss: 0.5938\n",
            "\u001b[1m 16/282\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Model: Transformer\n",
            "Accuracy: 0.7109, ROC-AUC: 0.6136, F1 Score: 0.6226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.97      0.83      6420\n",
            "           1       0.48      0.07      0.12      2591\n",
            "\n",
            "    accuracy                           0.71      9011\n",
            "   macro avg       0.60      0.52      0.47      9011\n",
            "weighted avg       0.65      0.71      0.62      9011\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 학습용 데이터와 테스트 데이터로 80:20 비율로 분리\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 언더샘플링 적용 (Churn 비율을 맞추기 위해 RandomUnderSampler 사용)\n",
        "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "train_x_resampled, train_y_resampled = undersampler.fit_resample(train_x, train_y)\n",
        "\n",
        "# 스케일링 (숫자형 변수만)\n",
        "scaler = StandardScaler()\n",
        "train_x_resampled[numeric_cols] = scaler.fit_transform(train_x_resampled[numeric_cols])\n",
        "test_x[numeric_cols] = scaler.transform(test_x[numeric_cols])\n",
        "\n",
        "# 데이터 타입을 float32로 변환 (텐서플로우 호환을 위해)\n",
        "train_x_resampled = train_x_resampled.astype('float32')\n",
        "test_x = test_x.astype('float32')\n",
        "\n",
        "# 데이터를 LSTM/GRU 입력 형식으로 변환 (샘플 수, 타임스텝, 특성 수)\n",
        "train_x_resampled = np.expand_dims(train_x_resampled, axis=1)\n",
        "test_x = np.expand_dims(test_x, axis=1)\n",
        "\n",
        "# 3. 모델 정의\n",
        "\n",
        "# 1. LSTM + Attention 모델\n",
        "def build_lstm_attention_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64, return_sequences=True)(inputs)\n",
        "    attention = Attention()([lstm_out, lstm_out])\n",
        "    flatten = Flatten()(attention)\n",
        "    dense_out = Dense(64, activation='relu')(flatten)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 2. BiLSTM + CNN 모델\n",
        "def build_bilstm_cnn_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
        "    cnn_out = Conv1D(64, kernel_size=1, activation='relu')(lstm_out)\n",
        "    cnn_out = MaxPooling1D(pool_size=1)(cnn_out)\n",
        "    cnn_out = GlobalAveragePooling1D()(cnn_out)\n",
        "    dense_out = Dense(64, activation='relu')(cnn_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. GRU + Attention 모델\n",
        "def build_gru_attention_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    gru_out = GRU(64, return_sequences=True)(inputs)\n",
        "    attention = Attention()([gru_out, gru_out])\n",
        "    flatten = Flatten()(attention)\n",
        "    dense_out = Dense(64, activation='relu')(flatten)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 4. Transformer 모델\n",
        "def build_transformer_model(input_shape, num_heads=8, ff_dim=64):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(input_shape[-1])(ff_output)\n",
        "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "    dense_out = Dense(64, activation='relu')(ff_output)\n",
        "    dense_out = Dropout(0.1)(dense_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 4. 모델 리스트 정의\n",
        "models = {\n",
        "    'LSTM + Attention': build_lstm_attention_model((1, train_x_resampled.shape[2])),\n",
        "    'BiLSTM + CNN': build_bilstm_cnn_model((1, train_x_resampled.shape[2])),\n",
        "    'GRU + Attention': build_gru_attention_model((1, train_x_resampled.shape[2])),\n",
        "    'Transformer': build_transformer_model((1, train_x_resampled.shape[2]))\n",
        "}\n",
        "\n",
        "# 5. 모델 학습 및 평가 결과 저장용 리스트 생성\n",
        "results = []\n",
        "\n",
        "# 6. 모델 학습 및 평가\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    model.fit(train_x_resampled, train_y_resampled, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "    # 테스트 데이터 평가\n",
        "    test_loss, test_accuracy = model.evaluate(test_x, test_y)\n",
        "    y_pred_proba = model.predict(test_x).ravel()\n",
        "    y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
        "\n",
        "    # 성능 지표 계산\n",
        "    accuracy = accuracy_score(test_y, y_pred)\n",
        "    roc_auc = roc_auc_score(test_y, y_pred_proba)\n",
        "    f1 = classification_report(test_y, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': model_name, 'Accuracy': accuracy, 'AUC': roc_auc, 'F1 Score': f1})\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}, F1 Score: {f1:.4f}\")\n",
        "    print(classification_report(test_y, y_pred))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 7. 결과 리스트를 데이터프레임으로 변환하여 성능 확인\n",
        "results_df1 = pd.DataFrame(results)\n",
        "results_df1.sort_values(by='F1 Score', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "WVX-fqipYFgF",
        "outputId": "99e270f5-b2fb-4aa2-c37a-99914808dc89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BiLSTM + CNN</td>\n",
              "      <td>0.694706</td>\n",
              "      <td>0.621330</td>\n",
              "      <td>0.658681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM + Attention</td>\n",
              "      <td>0.686605</td>\n",
              "      <td>0.619870</td>\n",
              "      <td>0.656821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GRU + Attention</td>\n",
              "      <td>0.702475</td>\n",
              "      <td>0.619522</td>\n",
              "      <td>0.642114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>0.710909</td>\n",
              "      <td>0.613646</td>\n",
              "      <td>0.622560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model  Accuracy       AUC  F1 Score\n",
              "1      BiLSTM + CNN  0.694706  0.621330  0.658681\n",
              "0  LSTM + Attention  0.686605  0.619870  0.656821\n",
              "2   GRU + Attention  0.702475  0.619522  0.642114\n",
              "3       Transformer  0.710909  0.613646  0.622560"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQLU1D9sWfPQ"
      },
      "source": [
        "###언더샘플링,10-fold\n",
        "*   Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "*   Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
        "나머지 추가하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycjvsCKeWiPV",
        "outputId": "b2dd380a-6751-4ae1-b1f3-bab914b0c092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "언더샘플링 후 클래스 분포:\n",
            "Churn\n",
            "0    12952\n",
            "1    12952\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Logistic Regression ===\n",
            "Fold 1 - Accuracy: 0.5681, ROC-AUC: 0.6059, F1 Score: 0.5593\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.59      0.58      1296\n",
            "           1       0.57      0.55      0.56      1295\n",
            "\n",
            "    accuracy                           0.57      2591\n",
            "   macro avg       0.57      0.57      0.57      2591\n",
            "weighted avg       0.57      0.57      0.57      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.5770, ROC-AUC: 0.6113, F1 Score: 0.5768\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.58      0.58      1296\n",
            "           1       0.58      0.58      0.58      1295\n",
            "\n",
            "    accuracy                           0.58      2591\n",
            "   macro avg       0.58      0.58      0.58      2591\n",
            "weighted avg       0.58      0.58      0.58      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.5646, ROC-AUC: 0.6002, F1 Score: 0.5618\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.57      0.57      1295\n",
            "           1       0.57      0.56      0.56      1296\n",
            "\n",
            "    accuracy                           0.56      2591\n",
            "   macro avg       0.56      0.56      0.56      2591\n",
            "weighted avg       0.56      0.56      0.56      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5731, ROC-AUC: 0.6067, F1 Score: 0.5632\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.60      0.58      1295\n",
            "           1       0.58      0.55      0.56      1296\n",
            "\n",
            "    accuracy                           0.57      2591\n",
            "   macro avg       0.57      0.57      0.57      2591\n",
            "weighted avg       0.57      0.57      0.57      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5803, ROC-AUC: 0.6084, F1 Score: 0.5772\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.59      0.58      1295\n",
            "           1       0.58      0.57      0.58      1295\n",
            "\n",
            "    accuracy                           0.58      2590\n",
            "   macro avg       0.58      0.58      0.58      2590\n",
            "weighted avg       0.58      0.58      0.58      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5861, ROC-AUC: 0.6154, F1 Score: 0.5796\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.60      0.59      1295\n",
            "           1       0.59      0.57      0.58      1295\n",
            "\n",
            "    accuracy                           0.59      2590\n",
            "   macro avg       0.59      0.59      0.59      2590\n",
            "weighted avg       0.59      0.59      0.59      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.5792, ROC-AUC: 0.6136, F1 Score: 0.5719\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.60      0.59      1295\n",
            "           1       0.58      0.56      0.57      1295\n",
            "\n",
            "    accuracy                           0.58      2590\n",
            "   macro avg       0.58      0.58      0.58      2590\n",
            "weighted avg       0.58      0.58      0.58      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.5826, ROC-AUC: 0.6211, F1 Score: 0.5746\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.60      0.59      1295\n",
            "           1       0.59      0.56      0.57      1295\n",
            "\n",
            "    accuracy                           0.58      2590\n",
            "   macro avg       0.58      0.58      0.58      2590\n",
            "weighted avg       0.58      0.58      0.58      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.5680, ROC-AUC: 0.5955, F1 Score: 0.5641\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.58      0.57      1295\n",
            "           1       0.57      0.56      0.56      1295\n",
            "\n",
            "    accuracy                           0.57      2590\n",
            "   macro avg       0.57      0.57      0.57      2590\n",
            "weighted avg       0.57      0.57      0.57      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.5622, ROC-AUC: 0.5882, F1 Score: 0.5662\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.55      0.56      1295\n",
            "           1       0.56      0.57      0.57      1295\n",
            "\n",
            "    accuracy                           0.56      2590\n",
            "   macro avg       0.56      0.56      0.56      2590\n",
            "weighted avg       0.56      0.56      0.56      2590\n",
            "\n",
            "\n",
            "Model: Logistic Regression\n",
            "Average Accuracy: 0.5741, Average ROC-AUC: 0.6066, Average F1 Score: 0.5695\n",
            "============================================================\n",
            "\n",
            "=== KNN ===\n",
            "Fold 1 - Accuracy: 0.5380, ROC-AUC: 0.5427, F1 Score: 0.5282\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55      1296\n",
            "           1       0.54      0.52      0.53      1295\n",
            "\n",
            "    accuracy                           0.54      2591\n",
            "   macro avg       0.54      0.54      0.54      2591\n",
            "weighted avg       0.54      0.54      0.54      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.5496, ROC-AUC: 0.5558, F1 Score: 0.5378\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.57      0.56      1296\n",
            "           1       0.55      0.52      0.54      1295\n",
            "\n",
            "    accuracy                           0.55      2591\n",
            "   macro avg       0.55      0.55      0.55      2591\n",
            "weighted avg       0.55      0.55      0.55      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.5407, ROC-AUC: 0.5531, F1 Score: 0.5337\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55      1295\n",
            "           1       0.54      0.53      0.53      1296\n",
            "\n",
            "    accuracy                           0.54      2591\n",
            "   macro avg       0.54      0.54      0.54      2591\n",
            "weighted avg       0.54      0.54      0.54      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5353, ROC-AUC: 0.5523, F1 Score: 0.5211\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55      1295\n",
            "           1       0.54      0.51      0.52      1296\n",
            "\n",
            "    accuracy                           0.54      2591\n",
            "   macro avg       0.54      0.54      0.53      2591\n",
            "weighted avg       0.54      0.54      0.53      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5286, ROC-AUC: 0.5397, F1 Score: 0.5126\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.56      0.54      1295\n",
            "           1       0.53      0.50      0.51      1295\n",
            "\n",
            "    accuracy                           0.53      2590\n",
            "   macro avg       0.53      0.53      0.53      2590\n",
            "weighted avg       0.53      0.53      0.53      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5174, ROC-AUC: 0.5262, F1 Score: 0.5117\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.53      0.52      1295\n",
            "           1       0.52      0.51      0.51      1295\n",
            "\n",
            "    accuracy                           0.52      2590\n",
            "   macro avg       0.52      0.52      0.52      2590\n",
            "weighted avg       0.52      0.52      0.52      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.5305, ROC-AUC: 0.5402, F1 Score: 0.5239\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.54      0.54      1295\n",
            "           1       0.53      0.52      0.52      1295\n",
            "\n",
            "    accuracy                           0.53      2590\n",
            "   macro avg       0.53      0.53      0.53      2590\n",
            "weighted avg       0.53      0.53      0.53      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.5309, ROC-AUC: 0.5463, F1 Score: 0.5211\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.55      0.54      1295\n",
            "           1       0.53      0.51      0.52      1295\n",
            "\n",
            "    accuracy                           0.53      2590\n",
            "   macro avg       0.53      0.53      0.53      2590\n",
            "weighted avg       0.53      0.53      0.53      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.5413, ROC-AUC: 0.5568, F1 Score: 0.5319\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55      1295\n",
            "           1       0.54      0.52      0.53      1295\n",
            "\n",
            "    accuracy                           0.54      2590\n",
            "   macro avg       0.54      0.54      0.54      2590\n",
            "weighted avg       0.54      0.54      0.54      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.5185, ROC-AUC: 0.5317, F1 Score: 0.5038\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.55      0.53      1295\n",
            "           1       0.52      0.49      0.50      1295\n",
            "\n",
            "    accuracy                           0.52      2590\n",
            "   macro avg       0.52      0.52      0.52      2590\n",
            "weighted avg       0.52      0.52      0.52      2590\n",
            "\n",
            "\n",
            "Model: KNN\n",
            "Average Accuracy: 0.5331, Average ROC-AUC: 0.5445, Average F1 Score: 0.5226\n",
            "============================================================\n",
            "\n",
            "=== Decision Tree ===\n",
            "Fold 1 - Accuracy: 0.5646, ROC-AUC: 0.5646, F1 Score: 0.5645\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.56      0.56      1296\n",
            "           1       0.56      0.56      0.56      1295\n",
            "\n",
            "    accuracy                           0.56      2591\n",
            "   macro avg       0.56      0.56      0.56      2591\n",
            "weighted avg       0.56      0.56      0.56      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.5504, ROC-AUC: 0.5504, F1 Score: 0.5555\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.54      0.55      1296\n",
            "           1       0.55      0.56      0.56      1295\n",
            "\n",
            "    accuracy                           0.55      2591\n",
            "   macro avg       0.55      0.55      0.55      2591\n",
            "weighted avg       0.55      0.55      0.55      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.5330, ROC-AUC: 0.5330, F1 Score: 0.5406\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.52      0.53      1295\n",
            "           1       0.53      0.55      0.54      1296\n",
            "\n",
            "    accuracy                           0.53      2591\n",
            "   macro avg       0.53      0.53      0.53      2591\n",
            "weighted avg       0.53      0.53      0.53      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5210, ROC-AUC: 0.5210, F1 Score: 0.5150\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.53      0.53      1295\n",
            "           1       0.52      0.51      0.52      1296\n",
            "\n",
            "    accuracy                           0.52      2591\n",
            "   macro avg       0.52      0.52      0.52      2591\n",
            "weighted avg       0.52      0.52      0.52      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5402, ROC-AUC: 0.5402, F1 Score: 0.5490\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.52      0.53      1295\n",
            "           1       0.54      0.56      0.55      1295\n",
            "\n",
            "    accuracy                           0.54      2590\n",
            "   macro avg       0.54      0.54      0.54      2590\n",
            "weighted avg       0.54      0.54      0.54      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5371, ROC-AUC: 0.5371, F1 Score: 0.5383\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.53      0.54      1295\n",
            "           1       0.54      0.54      0.54      1295\n",
            "\n",
            "    accuracy                           0.54      2590\n",
            "   macro avg       0.54      0.54      0.54      2590\n",
            "weighted avg       0.54      0.54      0.54      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.5429, ROC-AUC: 0.5429, F1 Score: 0.5443\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.54      0.54      1295\n",
            "           1       0.54      0.55      0.54      1295\n",
            "\n",
            "    accuracy                           0.54      2590\n",
            "   macro avg       0.54      0.54      0.54      2590\n",
            "weighted avg       0.54      0.54      0.54      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.5525, ROC-AUC: 0.5525, F1 Score: 0.5506\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.56      0.55      1295\n",
            "           1       0.55      0.55      0.55      1295\n",
            "\n",
            "    accuracy                           0.55      2590\n",
            "   macro avg       0.55      0.55      0.55      2590\n",
            "weighted avg       0.55      0.55      0.55      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.5514, ROC-AUC: 0.5514, F1 Score: 0.5464\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.56      0.56      1295\n",
            "           1       0.55      0.54      0.55      1295\n",
            "\n",
            "    accuracy                           0.55      2590\n",
            "   macro avg       0.55      0.55      0.55      2590\n",
            "weighted avg       0.55      0.55      0.55      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.5429, ROC-AUC: 0.5429, F1 Score: 0.5361\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55      1295\n",
            "           1       0.54      0.53      0.54      1295\n",
            "\n",
            "    accuracy                           0.54      2590\n",
            "   macro avg       0.54      0.54      0.54      2590\n",
            "weighted avg       0.54      0.54      0.54      2590\n",
            "\n",
            "\n",
            "Model: Decision Tree\n",
            "Average Accuracy: 0.5436, Average ROC-AUC: 0.5436, Average F1 Score: 0.5440\n",
            "============================================================\n",
            "\n",
            "=== Random Forest ===\n",
            "Fold 1 - Accuracy: 0.6256, ROC-AUC: 0.6713, F1 Score: 0.6303\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.61      0.62      1296\n",
            "           1       0.62      0.64      0.63      1295\n",
            "\n",
            "    accuracy                           0.63      2591\n",
            "   macro avg       0.63      0.63      0.63      2591\n",
            "weighted avg       0.63      0.63      0.63      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.6017, ROC-AUC: 0.6445, F1 Score: 0.5981\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.61      0.61      1296\n",
            "           1       0.60      0.59      0.60      1295\n",
            "\n",
            "    accuracy                           0.60      2591\n",
            "   macro avg       0.60      0.60      0.60      2591\n",
            "weighted avg       0.60      0.60      0.60      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.6032, ROC-AUC: 0.6474, F1 Score: 0.6147\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.57      0.59      1295\n",
            "           1       0.60      0.63      0.61      1296\n",
            "\n",
            "    accuracy                           0.60      2591\n",
            "   macro avg       0.60      0.60      0.60      2591\n",
            "weighted avg       0.60      0.60      0.60      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5963, ROC-AUC: 0.6303, F1 Score: 0.6002\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.59      0.59      1295\n",
            "           1       0.59      0.61      0.60      1296\n",
            "\n",
            "    accuracy                           0.60      2591\n",
            "   macro avg       0.60      0.60      0.60      2591\n",
            "weighted avg       0.60      0.60      0.60      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5946, ROC-AUC: 0.6364, F1 Score: 0.6008\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.58      0.59      1295\n",
            "           1       0.59      0.61      0.60      1295\n",
            "\n",
            "    accuracy                           0.59      2590\n",
            "   macro avg       0.59      0.59      0.59      2590\n",
            "weighted avg       0.59      0.59      0.59      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5961, ROC-AUC: 0.6364, F1 Score: 0.6023\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.58      0.59      1295\n",
            "           1       0.59      0.61      0.60      1295\n",
            "\n",
            "    accuracy                           0.60      2590\n",
            "   macro avg       0.60      0.60      0.60      2590\n",
            "weighted avg       0.60      0.60      0.60      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.6124, ROC-AUC: 0.6407, F1 Score: 0.6177\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.60      0.61      1295\n",
            "           1       0.61      0.63      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.6015, ROC-AUC: 0.6455, F1 Score: 0.6037\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.60      0.60      1295\n",
            "           1       0.60      0.61      0.60      1295\n",
            "\n",
            "    accuracy                           0.60      2590\n",
            "   macro avg       0.60      0.60      0.60      2590\n",
            "weighted avg       0.60      0.60      0.60      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.6127, ROC-AUC: 0.6490, F1 Score: 0.6182\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.60      0.61      1295\n",
            "           1       0.61      0.63      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.6050, ROC-AUC: 0.6547, F1 Score: 0.6164\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.59      1295\n",
            "           1       0.60      0.63      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.60      2590\n",
            "weighted avg       0.61      0.61      0.60      2590\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "Average Accuracy: 0.6049, Average ROC-AUC: 0.6456, Average F1 Score: 0.6102\n",
            "============================================================\n",
            "\n",
            "=== Gradient Boosting ===\n",
            "Fold 1 - Accuracy: 0.6187, ROC-AUC: 0.6704, F1 Score: 0.6357\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1296\n",
            "           1       0.61      0.67      0.64      1295\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.62      0.62      0.62      2591\n",
            "weighted avg       0.62      0.62      0.62      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.6048, ROC-AUC: 0.6517, F1 Score: 0.6213\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.56      0.59      1296\n",
            "           1       0.60      0.65      0.62      1295\n",
            "\n",
            "    accuracy                           0.60      2591\n",
            "   macro avg       0.61      0.60      0.60      2591\n",
            "weighted avg       0.61      0.60      0.60      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.6245, ROC-AUC: 0.6652, F1 Score: 0.6458\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.56      0.60      1295\n",
            "           1       0.61      0.68      0.65      1296\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.63      0.62      0.62      2591\n",
            "weighted avg       0.63      0.62      0.62      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.6129, ROC-AUC: 0.6525, F1 Score: 0.6338\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.56      0.59      1295\n",
            "           1       0.60      0.67      0.63      1296\n",
            "\n",
            "    accuracy                           0.61      2591\n",
            "   macro avg       0.61      0.61      0.61      2591\n",
            "weighted avg       0.61      0.61      0.61      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.6077, ROC-AUC: 0.6507, F1 Score: 0.6332\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.54      0.58      1295\n",
            "           1       0.59      0.68      0.63      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.6108, ROC-AUC: 0.6513, F1 Score: 0.6272\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.57      0.59      1295\n",
            "           1       0.60      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.6054, ROC-AUC: 0.6531, F1 Score: 0.6234\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.56      0.59      1295\n",
            "           1       0.60      0.65      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.60      2590\n",
            "weighted avg       0.61      0.61      0.60      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.6097, ROC-AUC: 0.6585, F1 Score: 0.6262\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.57      0.59      1295\n",
            "           1       0.60      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.6139, ROC-AUC: 0.6636, F1 Score: 0.6302\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.57      0.60      1295\n",
            "           1       0.60      0.66      0.63      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.6185, ROC-AUC: 0.6632, F1 Score: 0.6357\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1295\n",
            "           1       0.61      0.67      0.64      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Average Accuracy: 0.6127, Average ROC-AUC: 0.6580, Average F1 Score: 0.6313\n",
            "============================================================\n",
            "\n",
            "=== XGBoost ===\n",
            "Fold 1 - Accuracy: 0.6090, ROC-AUC: 0.6510, F1 Score: 0.6123\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.61      1296\n",
            "           1       0.61      0.62      0.61      1295\n",
            "\n",
            "    accuracy                           0.61      2591\n",
            "   macro avg       0.61      0.61      0.61      2591\n",
            "weighted avg       0.61      0.61      0.61      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.6056, ROC-AUC: 0.6452, F1 Score: 0.6096\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.59      0.60      1296\n",
            "           1       0.60      0.62      0.61      1295\n",
            "\n",
            "    accuracy                           0.61      2591\n",
            "   macro avg       0.61      0.61      0.61      2591\n",
            "weighted avg       0.61      0.61      0.61      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.6009, ROC-AUC: 0.6359, F1 Score: 0.6107\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.59      1295\n",
            "           1       0.60      0.63      0.61      1296\n",
            "\n",
            "    accuracy                           0.60      2591\n",
            "   macro avg       0.60      0.60      0.60      2591\n",
            "weighted avg       0.60      0.60      0.60      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5878, ROC-AUC: 0.6241, F1 Score: 0.5917\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.58      0.58      1295\n",
            "           1       0.59      0.60      0.59      1296\n",
            "\n",
            "    accuracy                           0.59      2591\n",
            "   macro avg       0.59      0.59      0.59      2591\n",
            "weighted avg       0.59      0.59      0.59      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5923, ROC-AUC: 0.6356, F1 Score: 0.6083\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.55      0.57      1295\n",
            "           1       0.59      0.63      0.61      1295\n",
            "\n",
            "    accuracy                           0.59      2590\n",
            "   macro avg       0.59      0.59      0.59      2590\n",
            "weighted avg       0.59      0.59      0.59      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5788, ROC-AUC: 0.6247, F1 Score: 0.5844\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.57      0.57      1295\n",
            "           1       0.58      0.59      0.58      1295\n",
            "\n",
            "    accuracy                           0.58      2590\n",
            "   macro avg       0.58      0.58      0.58      2590\n",
            "weighted avg       0.58      0.58      0.58      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.5938, ROC-AUC: 0.6281, F1 Score: 0.6051\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.57      0.58      1295\n",
            "           1       0.59      0.62      0.61      1295\n",
            "\n",
            "    accuracy                           0.59      2590\n",
            "   macro avg       0.59      0.59      0.59      2590\n",
            "weighted avg       0.59      0.59      0.59      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.6093, ROC-AUC: 0.6509, F1 Score: 0.6105\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.61      0.61      1295\n",
            "           1       0.61      0.61      0.61      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.6015, ROC-AUC: 0.6410, F1 Score: 0.6046\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.59      0.60      1295\n",
            "           1       0.60      0.61      0.60      1295\n",
            "\n",
            "    accuracy                           0.60      2590\n",
            "   macro avg       0.60      0.60      0.60      2590\n",
            "weighted avg       0.60      0.60      0.60      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.6093, ROC-AUC: 0.6461, F1 Score: 0.6164\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.59      0.60      1295\n",
            "           1       0.61      0.63      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "\n",
            "Model: XGBoost\n",
            "Average Accuracy: 0.5988, Average ROC-AUC: 0.6382, Average F1 Score: 0.6054\n",
            "============================================================\n",
            "\n",
            "=== LightGBM ===\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11656\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001458 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500021 -> initscore=0.000086\n",
            "[LightGBM] [Info] Start training from score 0.000086\n",
            "Fold 1 - Accuracy: 0.6206, ROC-AUC: 0.6738, F1 Score: 0.6333\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.59      0.61      1296\n",
            "           1       0.61      0.66      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.62      0.62      0.62      2591\n",
            "weighted avg       0.62      0.62      0.62      2591\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11656\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001562 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500021 -> initscore=0.000086\n",
            "[LightGBM] [Info] Start training from score 0.000086\n",
            "Fold 2 - Accuracy: 0.6156, ROC-AUC: 0.6541, F1 Score: 0.6306\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1296\n",
            "           1       0.61      0.66      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.62      0.62      0.61      2591\n",
            "weighted avg       0.62      0.62      0.61      2591\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11656, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499979 -> initscore=-0.000086\n",
            "[LightGBM] [Info] Start training from score -0.000086\n",
            "Fold 3 - Accuracy: 0.6245, ROC-AUC: 0.6676, F1 Score: 0.6414\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61      1295\n",
            "           1       0.61      0.67      0.64      1296\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.63      0.62      0.62      2591\n",
            "weighted avg       0.63      0.62      0.62      2591\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11656, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499979 -> initscore=-0.000086\n",
            "[LightGBM] [Info] Start training from score -0.000086\n",
            "Fold 4 - Accuracy: 0.6063, ROC-AUC: 0.6521, F1 Score: 0.6180\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.59      1295\n",
            "           1       0.60      0.64      0.62      1296\n",
            "\n",
            "    accuracy                           0.61      2591\n",
            "   macro avg       0.61      0.61      0.61      2591\n",
            "weighted avg       0.61      0.61      0.61      2591\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 5 - Accuracy: 0.6178, ROC-AUC: 0.6600, F1 Score: 0.6336\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1295\n",
            "           1       0.61      0.66      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3044\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 6 - Accuracy: 0.5973, ROC-AUC: 0.6475, F1 Score: 0.6098\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.57      0.58      1295\n",
            "           1       0.59      0.63      0.61      1295\n",
            "\n",
            "    accuracy                           0.60      2590\n",
            "   macro avg       0.60      0.60      0.60      2590\n",
            "weighted avg       0.60      0.60      0.60      2590\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 7 - Accuracy: 0.6081, ROC-AUC: 0.6498, F1 Score: 0.6256\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.56      0.59      1295\n",
            "           1       0.60      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3038\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 8 - Accuracy: 0.6201, ROC-AUC: 0.6673, F1 Score: 0.6284\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.61      1295\n",
            "           1       0.61      0.64      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3034\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 9 - Accuracy: 0.6297, ROC-AUC: 0.6644, F1 Score: 0.6407\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.60      0.62      1295\n",
            "           1       0.62      0.66      0.64      1295\n",
            "\n",
            "    accuracy                           0.63      2590\n",
            "   macro avg       0.63      0.63      0.63      2590\n",
            "weighted avg       0.63      0.63      0.63      2590\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3035\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 10 - Accuracy: 0.6286, ROC-AUC: 0.6736, F1 Score: 0.6442\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61      1295\n",
            "           1       0.62      0.67      0.64      1295\n",
            "\n",
            "    accuracy                           0.63      2590\n",
            "   macro avg       0.63      0.63      0.63      2590\n",
            "weighted avg       0.63      0.63      0.63      2590\n",
            "\n",
            "\n",
            "Model: LightGBM\n",
            "Average Accuracy: 0.6169, Average ROC-AUC: 0.6610, Average F1 Score: 0.6306\n",
            "============================================================\n",
            "\n",
            "=== CatBoost ===\n",
            "Fold 1 - Accuracy: 0.6241, ROC-AUC: 0.6756, F1 Score: 0.6316\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.62      1296\n",
            "           1       0.62      0.64      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.62      0.62      0.62      2591\n",
            "weighted avg       0.62      0.62      0.62      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.6183, ROC-AUC: 0.6597, F1 Score: 0.6308\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.58      0.60      1296\n",
            "           1       0.61      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2591\n",
            "   macro avg       0.62      0.62      0.62      2591\n",
            "weighted avg       0.62      0.62      0.62      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.6318, ROC-AUC: 0.6694, F1 Score: 0.6435\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.60      0.62      1295\n",
            "           1       0.62      0.66      0.64      1296\n",
            "\n",
            "    accuracy                           0.63      2591\n",
            "   macro avg       0.63      0.63      0.63      2591\n",
            "weighted avg       0.63      0.63      0.63      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.6129, ROC-AUC: 0.6550, F1 Score: 0.6222\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.59      0.60      1295\n",
            "           1       0.61      0.64      0.62      1296\n",
            "\n",
            "    accuracy                           0.61      2591\n",
            "   macro avg       0.61      0.61      0.61      2591\n",
            "weighted avg       0.61      0.61      0.61      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.6162, ROC-AUC: 0.6636, F1 Score: 0.6297\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.58      0.60      1295\n",
            "           1       0.61      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.6004, ROC-AUC: 0.6502, F1 Score: 0.6119\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.57      0.59      1295\n",
            "           1       0.59      0.63      0.61      1295\n",
            "\n",
            "    accuracy                           0.60      2590\n",
            "   macro avg       0.60      0.60      0.60      2590\n",
            "weighted avg       0.60      0.60      0.60      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.6116, ROC-AUC: 0.6567, F1 Score: 0.6224\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.58      0.60      1295\n",
            "           1       0.61      0.64      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.6243, ROC-AUC: 0.6720, F1 Score: 0.6338\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.60      0.61      1295\n",
            "           1       0.62      0.65      0.63      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.6127, ROC-AUC: 0.6653, F1 Score: 0.6231\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.59      0.60      1295\n",
            "           1       0.61      0.64      0.62      1295\n",
            "\n",
            "    accuracy                           0.61      2590\n",
            "   macro avg       0.61      0.61      0.61      2590\n",
            "weighted avg       0.61      0.61      0.61      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.6224, ROC-AUC: 0.6698, F1 Score: 0.6359\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.59      0.61      1295\n",
            "           1       0.61      0.66      0.64      1295\n",
            "\n",
            "    accuracy                           0.62      2590\n",
            "   macro avg       0.62      0.62      0.62      2590\n",
            "weighted avg       0.62      0.62      0.62      2590\n",
            "\n",
            "\n",
            "Model: CatBoost\n",
            "Average Accuracy: 0.6175, Average ROC-AUC: 0.6637, Average F1 Score: 0.6285\n",
            "============================================================\n",
            "\n",
            "=== MLP ===\n",
            "Fold 1 - Accuracy: 0.5832, ROC-AUC: 0.6134, F1 Score: 0.5970\n",
            "Detailed Classification Report for Fold 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.55      0.57      1296\n",
            "           1       0.58      0.62      0.60      1295\n",
            "\n",
            "    accuracy                           0.58      2591\n",
            "   macro avg       0.58      0.58      0.58      2591\n",
            "weighted avg       0.58      0.58      0.58      2591\n",
            "\n",
            "Fold 2 - Accuracy: 0.5662, ROC-AUC: 0.5891, F1 Score: 0.5784\n",
            "Detailed Classification Report for Fold 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.54      0.55      1296\n",
            "           1       0.56      0.60      0.58      1295\n",
            "\n",
            "    accuracy                           0.57      2591\n",
            "   macro avg       0.57      0.57      0.57      2591\n",
            "weighted avg       0.57      0.57      0.57      2591\n",
            "\n",
            "Fold 3 - Accuracy: 0.5589, ROC-AUC: 0.5894, F1 Score: 0.6013\n",
            "Detailed Classification Report for Fold 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.45      0.51      1295\n",
            "           1       0.55      0.67      0.60      1296\n",
            "\n",
            "    accuracy                           0.56      2591\n",
            "   macro avg       0.56      0.56      0.55      2591\n",
            "weighted avg       0.56      0.56      0.55      2591\n",
            "\n",
            "Fold 4 - Accuracy: 0.5596, ROC-AUC: 0.5925, F1 Score: 0.5819\n",
            "Detailed Classification Report for Fold 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.51      0.53      1295\n",
            "           1       0.55      0.61      0.58      1296\n",
            "\n",
            "    accuracy                           0.56      2591\n",
            "   macro avg       0.56      0.56      0.56      2591\n",
            "weighted avg       0.56      0.56      0.56      2591\n",
            "\n",
            "Fold 5 - Accuracy: 0.5552, ROC-AUC: 0.5845, F1 Score: 0.5355\n",
            "Detailed Classification Report for Fold 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.60      0.57      1295\n",
            "           1       0.56      0.51      0.54      1295\n",
            "\n",
            "    accuracy                           0.56      2590\n",
            "   macro avg       0.56      0.56      0.55      2590\n",
            "weighted avg       0.56      0.56      0.55      2590\n",
            "\n",
            "Fold 6 - Accuracy: 0.5749, ROC-AUC: 0.6014, F1 Score: 0.5573\n",
            "Detailed Classification Report for Fold 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.61      0.59      1295\n",
            "           1       0.58      0.54      0.56      1295\n",
            "\n",
            "    accuracy                           0.57      2590\n",
            "   macro avg       0.58      0.57      0.57      2590\n",
            "weighted avg       0.58      0.57      0.57      2590\n",
            "\n",
            "Fold 7 - Accuracy: 0.5687, ROC-AUC: 0.5918, F1 Score: 0.5348\n",
            "Detailed Classification Report for Fold 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.64      0.60      1295\n",
            "           1       0.58      0.50      0.53      1295\n",
            "\n",
            "    accuracy                           0.57      2590\n",
            "   macro avg       0.57      0.57      0.57      2590\n",
            "weighted avg       0.57      0.57      0.57      2590\n",
            "\n",
            "Fold 8 - Accuracy: 0.5857, ROC-AUC: 0.6063, F1 Score: 0.5937\n",
            "Detailed Classification Report for Fold 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.57      0.58      1295\n",
            "           1       0.58      0.61      0.59      1295\n",
            "\n",
            "    accuracy                           0.59      2590\n",
            "   macro avg       0.59      0.59      0.59      2590\n",
            "weighted avg       0.59      0.59      0.59      2590\n",
            "\n",
            "Fold 9 - Accuracy: 0.5653, ROC-AUC: 0.6008, F1 Score: 0.4964\n",
            "Detailed Classification Report for Fold 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.70      0.62      1295\n",
            "           1       0.59      0.43      0.50      1295\n",
            "\n",
            "    accuracy                           0.57      2590\n",
            "   macro avg       0.57      0.57      0.56      2590\n",
            "weighted avg       0.57      0.57      0.56      2590\n",
            "\n",
            "Fold 10 - Accuracy: 0.5707, ROC-AUC: 0.5942, F1 Score: 0.5454\n",
            "Detailed Classification Report for Fold 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.63      0.59      1295\n",
            "           1       0.58      0.52      0.55      1295\n",
            "\n",
            "    accuracy                           0.57      2590\n",
            "   macro avg       0.57      0.57      0.57      2590\n",
            "weighted avg       0.57      0.57      0.57      2590\n",
            "\n",
            "\n",
            "Model: MLP\n",
            "Average Accuracy: 0.5688, Average ROC-AUC: 0.5964, Average F1 Score: 0.5622\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 언더샘플링 적용 (다수 클래스의 샘플을 줄여서 클래스 비율 맞춤)\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
        "\n",
        "# 언더샘플링 후 클래스 분포 확인\n",
        "print(\"\\n언더샘플링 후 클래스 분포:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# 스케일링 (숫자형 변수만)\n",
        "scaler = StandardScaler()\n",
        "X_resampled[numeric_cols] = scaler.fit_transform(X_resampled[numeric_cols])\n",
        "\n",
        "# 10-fold 교차 검증 설정\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# 모델 리스트 정의\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "# 모델 학습 및 평가 결과 저장용 리스트 생성\n",
        "results = []\n",
        "\n",
        "# 각 모델에 대해 학습 및 평가\n",
        "for model_name, classifier in models.items():\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "\n",
        "    # 교차 검증 정확도, ROC-AUC, F1 Score 저장용 리스트\n",
        "    accuracy_list = []\n",
        "    roc_auc_list = []\n",
        "    f1_list = []\n",
        "    detailed_results = []\n",
        "    fold = 1\n",
        "\n",
        "    # 각 폴드별로 학습 및 평가\n",
        "    for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
        "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
        "        y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
        "\n",
        "        # 모델 학습\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # 예측 생성\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        y_pred_proba = classifier.predict_proba(X_test)[:, 1] if hasattr(classifier, \"predict_proba\") else np.zeros(len(y_test))\n",
        "\n",
        "        # 평가 지표 계산\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        # 세부 평가 지표 출력\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        detailed_report = classification_report(y_test, y_pred)\n",
        "\n",
        "        # 리스트에 저장\n",
        "        accuracy_list.append(accuracy)\n",
        "        roc_auc_list.append(roc_auc)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        # 폴드 결과 출력\n",
        "        print(f\"Fold {fold} - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}, F1 Score: {f1:.4f}\")\n",
        "        print(f\"Detailed Classification Report for Fold {fold}:\\n{detailed_report}\")\n",
        "        fold += 1\n",
        "\n",
        "    # 평균 지표 계산\n",
        "    avg_accuracy = np.mean(accuracy_list)\n",
        "    avg_roc_auc = np.mean(roc_auc_list)\n",
        "    avg_f1 = np.mean(f1_list)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': model_name, 'Accuracy': avg_accuracy, 'AUC': avg_roc_auc, 'F1 Score': avg_f1})\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy:.4f}, Average ROC-AUC: {avg_roc_auc:.4f}, Average F1 Score: {avg_f1:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 결과 리스트를 데이터프레임으로 변환하여 성능 확인\n",
        "results_10U = pd.DataFrame(results)\n",
        "results_10U.sort_values(by='F1 Score', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4PB5P1XnJimw",
        "outputId": "b4311a84-209f-41f0-f92a-0be6238ab82e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.612685</td>\n",
              "      <td>0.658030</td>\n",
              "      <td>0.631258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.616855</td>\n",
              "      <td>0.661019</td>\n",
              "      <td>0.630557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.617472</td>\n",
              "      <td>0.663744</td>\n",
              "      <td>0.628489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.604926</td>\n",
              "      <td>0.645615</td>\n",
              "      <td>0.610233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.598826</td>\n",
              "      <td>0.638249</td>\n",
              "      <td>0.605362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.574120</td>\n",
              "      <td>0.606639</td>\n",
              "      <td>0.569461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.568831</td>\n",
              "      <td>0.596351</td>\n",
              "      <td>0.562171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.543584</td>\n",
              "      <td>0.543585</td>\n",
              "      <td>0.544037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.533082</td>\n",
              "      <td>0.544472</td>\n",
              "      <td>0.522575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy       AUC  F1 Score\n",
              "4    Gradient Boosting  0.612685  0.658030  0.631258\n",
              "6             LightGBM  0.616855  0.661019  0.630557\n",
              "7             CatBoost  0.617472  0.663744  0.628489\n",
              "3        Random Forest  0.604926  0.645615  0.610233\n",
              "5              XGBoost  0.598826  0.638249  0.605362\n",
              "0  Logistic Regression  0.574120  0.606639  0.569461\n",
              "8                  MLP  0.568831  0.596351  0.562171\n",
              "2        Decision Tree  0.543584  0.543585  0.544037\n",
              "1                  KNN  0.533082  0.544472  0.522575"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_10U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGwsF-_sqiEm"
      },
      "source": [
        "###10-fold 딥러닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-eyFrlLTOjX",
        "outputId": "a3a36e43-2050-4aa7-e6f2-bbc7be7f82a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "언더샘플링 후 클래스 분포:\n",
            "Churn\n",
            "0    12952\n",
            "1    12952\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "c:\\Users\\USER\\miniconda3\\envs\\AIproject\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "\n",
            "=== KNN ===\n",
            "\n",
            "=== Decision Tree ===\n",
            "\n",
            "=== Random Forest ===\n",
            "\n",
            "=== Gradient Boosting ===\n",
            "\n",
            "=== XGBoost ===\n",
            "\n",
            "=== LightGBM ===\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11656\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500021 -> initscore=0.000086\n",
            "[LightGBM] [Info] Start training from score 0.000086\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11656\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500021 -> initscore=0.000086\n",
            "[LightGBM] [Info] Start training from score 0.000086\n",
            "[LightGBM] [Info] Number of positive: 11656, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499979 -> initscore=-0.000086\n",
            "[LightGBM] [Info] Start training from score -0.000086\n",
            "[LightGBM] [Info] Number of positive: 11656, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3039\n",
            "[LightGBM] [Info] Number of data points in the train set: 23313, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499979 -> initscore=-0.000086\n",
            "[LightGBM] [Info] Start training from score -0.000086\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001256 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3044\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3041\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001197 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3038\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3034\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 11657, number of negative: 11657\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3035\n",
            "[LightGBM] [Info] Number of data points in the train set: 23314, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "\n",
            "=== CatBoost ===\n",
            "\n",
            "=== MLP_Sklearn ===\n",
            "\n",
            "=== ANN ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "=== DNN ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "=== CNN ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== RNN ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== SLP ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step\n",
            "\n",
            "=== MLP_NN ===\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step\n"
          ]
        }
      ],
      "source": [
        "# 언더샘플링 적용 (다수 클래스의 샘플을 줄여서 클래스 비율 맞춤)\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
        "\n",
        "# 언더샘플링 후 클래스 분포 확인\n",
        "print(\"\\n언더샘플링 후 클래스 분포:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# 스케일링 (숫자형 변수만)\n",
        "scaler = StandardScaler()\n",
        "X_resampled[numeric_cols] = scaler.fit_transform(X_resampled[numeric_cols])\n",
        "\n",
        "# 데이터를 신경망 모델 입력에 맞게 변환하고 float32로 변환\n",
        "X_resampled_nn = np.expand_dims(X_resampled.astype('float32'), axis=1)  # CNN, RNN 모델용 데이터 변환\n",
        "\n",
        "# 10-fold 교차 검증 설정\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# 신경망 모델 정의\n",
        "def build_ann_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_dnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 수정된 CNN 모델\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=1, activation='relu', input_shape=input_shape, padding='same'))  # 수정된 부분\n",
        "    model.add(Flatten())  # MaxPooling1D 제거\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, return_sequences=False, input_shape=input_shape))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_slp_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, activation='sigmoid', input_shape=input_shape))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_mlp_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 모델 리스트 정의\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"MLP_Sklearn\": MLPClassifier(max_iter=1000),\n",
        "    \"ANN\": build_ann_model((1, X_resampled.shape[1])),\n",
        "    \"DNN\": build_dnn_model((1, X_resampled.shape[1])),\n",
        "    \"CNN\": build_cnn_model((1, X_resampled_nn.shape[2])),\n",
        "    \"RNN\": build_rnn_model((1, X_resampled_nn.shape[2])),\n",
        "    \"SLP\": build_slp_model((1, X_resampled.shape[1])),\n",
        "    \"MLP_NN\": build_mlp_model((1, X_resampled.shape[1]))\n",
        "}\n",
        "\n",
        "# 모델 학습 및 평가 결과 저장용 리스트 생성\n",
        "results = []\n",
        "\n",
        "# 각 모델에 대해 학습 및 평가\n",
        "for model_name, classifier in models.items():\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "\n",
        "    accuracy_list = []\n",
        "    roc_auc_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
        "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
        "        y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
        "\n",
        "        if model_name in ['ANN', 'DNN', 'CNN', 'RNN', 'SLP', 'MLP_NN']:\n",
        "            X_train_nn, X_test_nn = np.expand_dims(X_train.astype('float32'), axis=1), np.expand_dims(X_test.astype('float32'), axis=1)\n",
        "            classifier.fit(X_train_nn, y_train, epochs=10, batch_size=64, verbose=0)\n",
        "            y_pred_proba = classifier.predict(X_test_nn).ravel()\n",
        "            y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
        "        else:\n",
        "            classifier.fit(X_train, y_train)\n",
        "            y_pred = classifier.predict(X_test)\n",
        "            y_pred_proba = classifier.predict_proba(X_test)[:, 1] if hasattr(classifier, \"predict_proba\") else np.zeros(len(y_test))\n",
        "\n",
        "        # 평가 지표 계산\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        # 리스트에 저장\n",
        "        accuracy_list.append(accuracy)\n",
        "        roc_auc_list.append(roc_auc)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    # 평균 지표 계산\n",
        "    avg_accuracy = np.mean(accuracy_list)\n",
        "    avg_roc_auc = np.mean(roc_auc_list)\n",
        "    avg_f1 = np.mean(f1_list)\n",
        "\n",
        "    # 결과 저장\n",
        "    results.append({'Model': model_name, 'Accuracy': avg_accuracy, 'AUC': avg_roc_auc, 'F1 Score': avg_f1})\n",
        "\n",
        "# 결과 출력\n",
        "results_df_total = pd.DataFrame(results)\n",
        "results_df_total.sort_values(by='F1 Score', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "R_oohJFkojwR",
        "outputId": "54f8a810-1f8c-40dc-d04b-57f1bf8a794e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DNN</td>\n",
              "      <td>0.721716</td>\n",
              "      <td>0.799055</td>\n",
              "      <td>0.723026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ANN</td>\n",
              "      <td>0.645311</td>\n",
              "      <td>0.701373</td>\n",
              "      <td>0.653069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>MLP_NN</td>\n",
              "      <td>0.643496</td>\n",
              "      <td>0.702019</td>\n",
              "      <td>0.650809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CNN</td>\n",
              "      <td>0.634771</td>\n",
              "      <td>0.691323</td>\n",
              "      <td>0.645505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RNN</td>\n",
              "      <td>0.642184</td>\n",
              "      <td>0.701702</td>\n",
              "      <td>0.638481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.612724</td>\n",
              "      <td>0.658083</td>\n",
              "      <td>0.631309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.616855</td>\n",
              "      <td>0.661019</td>\n",
              "      <td>0.630557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.617472</td>\n",
              "      <td>0.663744</td>\n",
              "      <td>0.628489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.606663</td>\n",
              "      <td>0.645126</td>\n",
              "      <td>0.610776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.598826</td>\n",
              "      <td>0.638249</td>\n",
              "      <td>0.605362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP_Sklearn</td>\n",
              "      <td>0.568445</td>\n",
              "      <td>0.595291</td>\n",
              "      <td>0.574677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SLP</td>\n",
              "      <td>0.575780</td>\n",
              "      <td>0.606185</td>\n",
              "      <td>0.574547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.574120</td>\n",
              "      <td>0.606639</td>\n",
              "      <td>0.569461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.542696</td>\n",
              "      <td>0.542696</td>\n",
              "      <td>0.542831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.533082</td>\n",
              "      <td>0.544472</td>\n",
              "      <td>0.522575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Model  Accuracy       AUC  F1 Score\n",
              "10                  DNN  0.721716  0.799055  0.723026\n",
              "9                   ANN  0.645311  0.701373  0.653069\n",
              "14               MLP_NN  0.643496  0.702019  0.650809\n",
              "11                  CNN  0.634771  0.691323  0.645505\n",
              "12                  RNN  0.642184  0.701702  0.638481\n",
              "4     Gradient Boosting  0.612724  0.658083  0.631309\n",
              "6              LightGBM  0.616855  0.661019  0.630557\n",
              "7              CatBoost  0.617472  0.663744  0.628489\n",
              "3         Random Forest  0.606663  0.645126  0.610776\n",
              "5               XGBoost  0.598826  0.638249  0.605362\n",
              "8           MLP_Sklearn  0.568445  0.595291  0.574677\n",
              "13                  SLP  0.575780  0.606185  0.574547\n",
              "0   Logistic Regression  0.574120  0.606639  0.569461\n",
              "2         Decision Tree  0.542696  0.542696  0.542831\n",
              "1                   KNN  0.533082  0.544472  0.522575"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QzUjVmmokdn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "AIproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
